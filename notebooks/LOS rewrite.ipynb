{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite of the \"Baselines for mortality and LOS prediction\" notebooks.\n",
    "#### This notebook contains code to predict LOS using GRU-D, Random Forest and Logistic Regression in a simple notebook with explanations.\n",
    "\n",
    "Author(s): Tomass Wilson, thwmi@kth.se\n",
    "\n",
    "Credit: Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Michael C. Hughes, Tristan Naumann, \n",
    "and Marzyeh Ghassemi. MIMIC-Extract: A Data Extraction, Preprocessing, and Representation \n",
    "Pipeline for MIMIC-III. arXiv:1907.08322. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from mmd_grud_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "DATA_FILEPATH     = os.path.join(dirname, \"..\", \"data\", \"all_hourly_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21689877af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '0'  # set this to the ID(s) of your most powerful GPU(s)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_full = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create True/False mappings of the targets LOS 3/7 days\n",
    "Also we create a list of subjects that all have lengths of stay that are at least 30 hours long.\n",
    "\n",
    "Ys is a dataframe containing the target results for each subject (eg that they did stay for longer than 3 days).\n",
    "\n",
    "data_df contains the metrics which we want to evaluate to predict length of stay. They are also the ones on which differential privacy will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All patients in ys ahave been in icu at some point for at least 30 hours\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3  # True/False column\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "\n",
    "# Get only the medical data of the chosen subjects\n",
    "data_df = data_full[\n",
    "    (data_full.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (data_full.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "]\n",
    "\n",
    "# Collect the subject id's\n",
    "subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (data_df, Ys)]\n",
    "subjects = set(subj_idx)\n",
    "assert subjects == set(Ys_subj_idx), \"Subject ID pools differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, dev and test fractions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac, dev_frac, test_frac = 0.7, 0.1, 0.2\n",
    "np.random.seed(SEED)\n",
    "subjects, N = np.random.permutation(list(subjects)), len(subjects)\n",
    "N_train, N_dev, N_test = int(train_frac * N), int(dev_frac * N), int(test_frac * N)\n",
    "train_subj = subjects[:N_train]\n",
    "dev_subj   = subjects[N_train:N_train + N_dev]\n",
    "test_subj  = subjects[N_train+N_dev:]\n",
    "\n",
    "# Create train, dev and test fractions for data and Ys\n",
    "[(data_train, data_dev, data_test), (Ys_train, Ys_dev, Ys_test)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)].copy() for s in (train_subj, dev_subj, test_subj)] \\\n",
    "    for df in (data_df, Ys)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smthng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "data_means = data_train.loc[:, idx[:,'mean']].mean(axis=0)\n",
    "data_stds = data_train.loc[:, idx[:,'mean']].std(axis=0)\n",
    "\n",
    "data_train.loc[:, idx[:,'mean']] = (data_train.loc[:, idx[:,'mean']] - data_means)/data_stds\n",
    "data_dev.loc[:, idx[:,'mean']] = (data_dev.loc[:, idx[:,'mean']] - data_means)/data_stds\n",
    "data_test.loc[:, idx[:,'mean']] = (data_test.loc[:, idx[:,'mean']] - data_means)/data_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smthng else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    if len(df.columns.names) > 2: \n",
    "        df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]].copy()\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out\n",
    "\n",
    "data_train, data_dev, data_test = [\n",
    "    simple_imputer(df) for df in (data_train, data_dev, data_test)\n",
    "]\n",
    "\n",
    "data_flat_train, data_flat_dev, data_flat_test = [\n",
    "    df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "        data_train, data_dev, data_test\n",
    "    )\n",
    "]\n",
    "\n",
    "for df in data_train, data_dev, data_test: \n",
    "    assert not df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "\n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l1', 'l2']),\n",
    "    'solver': Choice(['liblinear', 'lbfgs']),\n",
    "    'max_iter': Choice([100, 500])\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "for i in range(N):\n",
    "    if LR_hyperparams_list[i]['solver'] == 'lbfgs': LR_hyperparams_list[i]['penalty'] = 'l2'\n",
    "\n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 500),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparams_list = RF_dist.rvs(N)\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95),\n",
    "    'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "    'num_epochs': ss.randint(15, 150),\n",
    "    'patience': ss.randint(3, 7),\n",
    "    'batch_size': ss.randint(35, 65),\n",
    "    'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "    'seed': ss.randint(1, 10000),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # Declare results dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_flat_train, X_flat_dev, X_flat_test, target):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_flat_train, Ys_train[target])\n",
    "        s = roc_auc_score(Ys_dev[target], M.predict_proba(X_flat_dev)[:, 1])\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(pd.concat((X_flat_train, X_flat_dev)), pd.concat((Ys_train, Ys_dev))[target])\n",
    "    y_true  = Ys_test[target]\n",
    "    y_score = best_M.predict_proba(X_flat_test)[:, 1]\n",
    "    y_pred  = best_M.predict(X_flat_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return best_M, best_hyperparams, auc, auprc, acc, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model RF on target los_7 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8})\n",
      "New Best Score: 76.67 @ hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8}\n",
      "On sample 2 / 15 (hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4})\n",
      "New Best Score: 77.08 @ hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4}\n",
      "On sample 3 / 15 (hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7})\n",
      "New Best Score: 77.60 @ hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "On sample 4 / 15 (hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22})\n",
      "On sample 5 / 15 (hyperparams = {'n_estimators': 305, 'max_depth': 7, 'min_samples_split': 63, 'min_samples_leaf': 4})\n",
      "On sample 6 / 15 (hyperparams = {'n_estimators': 443, 'max_depth': 4, 'min_samples_split': 24, 'min_samples_leaf': 5})\n",
      "On sample 7 / 15 (hyperparams = {'n_estimators': 253, 'max_depth': 6, 'min_samples_split': 59, 'min_samples_leaf': 25})\n",
      "On sample 8 / 15 (hyperparams = {'n_estimators': 183, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 44})\n",
      "On sample 9 / 15 (hyperparams = {'n_estimators': 385, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 13})\n",
      "On sample 10 / 15 (hyperparams = {'n_estimators': 498, 'max_depth': 4, 'min_samples_split': 62, 'min_samples_leaf': 27})\n",
      "On sample 11 / 15 (hyperparams = {'n_estimators': 194, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 17})\n",
      "On sample 12 / 15 (hyperparams = {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 46})\n",
      "New Best Score: 78.15 @ hyperparams = {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 46}\n",
      "On sample 13 / 15 (hyperparams = {'n_estimators': 121, 'max_depth': 8, 'min_samples_split': 49, 'min_samples_leaf': 42})\n",
      "On sample 14 / 15 (hyperparams = {'n_estimators': 287, 'max_depth': 4, 'min_samples_split': 74, 'min_samples_leaf': 19})\n",
      "On sample 15 / 15 (hyperparams = {'n_estimators': 440, 'max_depth': 6, 'min_samples_split': 32, 'min_samples_leaf': 16})\n",
      "Final results for model RF on target los_7 with representation data\n",
      "(0.7606418270608, 0.19693435584132754, 0.9233820459290187, 0.0)\n",
      "Running model RF on target los_3 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8})\n",
      "New Best Score: 70.67 @ hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8}\n",
      "On sample 2 / 15 (hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4})\n",
      "New Best Score: 71.38 @ hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4}\n",
      "On sample 3 / 15 (hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7})\n",
      "New Best Score: 72.28 @ hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "On sample 4 / 15 (hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22})\n",
      "New Best Score: 72.85 @ hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22}\n",
      "On sample 5 / 15 (hyperparams = {'n_estimators': 305, 'max_depth': 7, 'min_samples_split': 63, 'min_samples_leaf': 4})\n",
      "On sample 6 / 15 (hyperparams = {'n_estimators': 443, 'max_depth': 4, 'min_samples_split': 24, 'min_samples_leaf': 5})\n"
     ]
    }
   ],
   "source": [
    "for model_name, model, hyperparams_list in [\n",
    "    ('RF', RandomForestClassifier, RF_hyperparams_list), ('LR', LogisticRegression, LR_hyperparams_list)\n",
    "]:\n",
    "    if model_name not in results: \n",
    "        results[model_name] = {}\n",
    "    for t in ['los_7', 'los_3']:\n",
    "        if t not in results[model_name]: \n",
    "            results[model_name][t] = {}\n",
    "        \n",
    "        if \"data\" in results[model_name][t]:\n",
    "            print(\"Finished model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "            if RERUN: \n",
    "                h = results[model_name][t][\"data\"][1]\n",
    "                results[model_name][t][\"data\"] = run_only_final(model, h, data_flat_train, data_flat_dev, data_flat_test, t)\n",
    "\n",
    "                print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "                print(results[model_name][t][\"data\"][2:])\n",
    "\n",
    "                with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)\n",
    "            continue\n",
    "\n",
    "        print(\"Running model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        results[model_name][t][\"data\"] = run_basic(\n",
    "            model, hyperparams_list, data_flat_train, data_flat_dev, data_flat_test, t\n",
    "        )\n",
    "        print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        print(results[model_name][t][\"data\"][2:])  # auc, auprc, acc, F1\n",
    "        # with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name       = 'GRU-D'\n",
    "hyperparams_list = GRU_D_hyperparams_list\n",
    "RERUN            = False\n",
    "\n",
    "if model_name not in results: \n",
    "    results[model_name] = {}\n",
    "\n",
    "for t in ['los_3', 'los_7']:\n",
    "    if t not in results[model_name]: \n",
    "        results[model_name][t] = {}\n",
    "    n, X_train, X_dev, X_test = ('data', data_train, data_dev, data_test)\n",
    "    \n",
    "    print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    X_mean = np.nanmean(\n",
    "        to_3D_tensor(\n",
    "            X_train.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "            np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "        ),\n",
    "        axis=0, keepdims=True\n",
    "    ).transpose([0, 2, 1])\n",
    "    base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "    if n in results[model_name][t]:\n",
    "        if not RERUN: \n",
    "            print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "            print(results[model_name][t][n])\n",
    "            continue\n",
    "        best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "        print(\"Loading best hyperparams\", best_hyperparams)\n",
    "    else:\n",
    "        best_s, best_hyperparams = -np.Inf, None\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "            early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "            print(type(batch_size))\n",
    "            batch_size = int(batch_size)\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[\n",
    "                Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "            ]\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(\n",
    "                X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "            )\n",
    "            dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "            test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model = GRUD(**model_hyperparams)\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "            probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "            probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "            labels_dev        = np.concatenate(labels_dev)\n",
    "            print(probabilities_dev)\n",
    "            print(labels_dev)\n",
    "            s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "            if s > best_s:\n",
    "                best_s, best_hyperparams = s, hyperparams\n",
    "                print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "                \n",
    "    ## Test\n",
    "    np.random.seed(seed)\n",
    "    early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "    X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "\n",
    "    all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "    N_early_stop = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "    X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "    X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "\n",
    "    y_score = np.concatenate(probabilities_test)[:, 1]\n",
    "    y_pred  = np.argmax(probabilities_test)\n",
    "    y_true  = np.concatenate(labels_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    print(auc, auprc, acc, F1)\n",
    "\n",
    "    results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "    # with open('/scratch/mmd/extraction_baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
