{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite of the \"Baselines for mortality and LOS prediction\" notebooks.\n",
    "#### This notebook contains code to predict LOS using GRU-D, Random Forest and Logistic Regression in a simple notebook with explanations.\n",
    "\n",
    "Author(s): Tomass Wilson, thwmi@kth.se\n",
    "\n",
    "Credit: Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Michael C. Hughes, Tristan Naumann, \n",
    "and Marzyeh Ghassemi. MIMIC-Extract: A Data Extraction, Preprocessing, and Representation \n",
    "Pipeline for MIMIC-III. arXiv:1907.08322. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from mmd_grud_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "DATA_FILEPATH     = os.path.join(dirname, \"..\", \"data\", \"all_hourly_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2babcc65070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 2\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '0'  # set this to the ID(s) of your most powerful GPU(s)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_full = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create True/False mappings of the targets LOS 3/7 days\n",
    "Also we create a list of subjects that all have lengths of stay that are at least 30 hours long.\n",
    "\n",
    "Ys is a dataframe containing the target results for each subject (eg that they did stay for longer than 3 days).\n",
    "\n",
    "data_df contains the metrics which we want to evaluate to predict length of stay. They are also the ones on which differential privacy will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All patients in ys ahave been in icu at some point for at least 30 hours\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3  # True/False column\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "\n",
    "# Get only the medical data of the chosen subjects\n",
    "data_df = data_full[\n",
    "    (data_full.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (data_full.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "]\n",
    "\n",
    "# Collect the subject id's\n",
    "subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (data_df, Ys)]\n",
    "subjects = set(subj_idx)\n",
    "assert subjects == set(Ys_subj_idx), \"Subject ID pools differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, dev and test fractions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac, dev_frac, test_frac = 0.7, 0.1, 0.2\n",
    "np.random.seed(SEED)\n",
    "subjects, N = np.random.permutation(list(subjects)), len(subjects)\n",
    "N_train, N_dev, N_test = int(train_frac * N), int(dev_frac * N), int(test_frac * N)\n",
    "train_subj = subjects[:N_train]\n",
    "dev_subj   = subjects[N_train:N_train + N_dev]\n",
    "test_subj  = subjects[N_train+N_dev:]\n",
    "\n",
    "# Create train, dev and test fractions for data and Ys\n",
    "[(data_train, data_dev, data_test), (Ys_train, Ys_dev, Ys_test)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)].copy() for s in (train_subj, dev_subj, test_subj)] \\\n",
    "    for df in (data_df, Ys)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smthng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "data_means = data_train.loc[:, idx[:,'mean']].mean(axis=0)\n",
    "data_stds = data_train.loc[:, idx[:,'mean']].std(axis=0)\n",
    "\n",
    "data_train.loc[:, idx[:,'mean']] = (data_train.loc[:, idx[:,'mean']] - data_means)/data_stds\n",
    "data_dev.loc[:, idx[:,'mean']] = (data_dev.loc[:, idx[:,'mean']] - data_means)/data_stds\n",
    "data_test.loc[:, idx[:,'mean']] = (data_test.loc[:, idx[:,'mean']] - data_means)/data_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smthng else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    if len(df.columns.names) > 2: \n",
    "        df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]].copy()\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out\n",
    "\n",
    "data_train, data_dev, data_test = [\n",
    "    simple_imputer(df) for df in (data_train, data_dev, data_test)\n",
    "]\n",
    "\n",
    "data_flat_train, data_flat_dev, data_flat_test = [\n",
    "    df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "        data_train, data_dev, data_test\n",
    "    )\n",
    "]\n",
    "\n",
    "for df in data_train, data_dev, data_test: \n",
    "    assert not df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "\n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l1', 'l2']),\n",
    "    'solver': Choice(['liblinear', 'lbfgs']),\n",
    "    'max_iter': Choice([100, 500])\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "for i in range(N):\n",
    "    if LR_hyperparams_list[i]['solver'] == 'lbfgs': LR_hyperparams_list[i]['penalty'] = 'l2'\n",
    "\n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 500),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparams_list = RF_dist.rvs(N)\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95),\n",
    "    'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "    'num_epochs': ss.randint(60, 150),\n",
    "    'patience': ss.randint(5, 9),\n",
    "    'batch_size': Choice([64, 128, 256, 512]),\n",
    "    'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "    'seed': ss.randint(1, 10000),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # Declare results dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_flat_train, X_flat_dev, X_flat_test, target):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_flat_train, Ys_train[target])\n",
    "        s = roc_auc_score(Ys_dev[target], M.predict_proba(X_flat_dev)[:, 1])\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(pd.concat((X_flat_train, X_flat_dev)), pd.concat((Ys_train, Ys_dev))[target])\n",
    "    y_true  = Ys_test[target]\n",
    "    y_score = best_M.predict_proba(X_flat_test)[:, 1]\n",
    "    y_pred  = best_M.predict(X_flat_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return best_M, best_hyperparams, auc, auprc, acc, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model RF on target los_7 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8})\n",
      "New Best Score: 76.67 @ hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8}\n",
      "On sample 2 / 15 (hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4})\n",
      "New Best Score: 77.08 @ hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4}\n",
      "On sample 3 / 15 (hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7})\n",
      "New Best Score: 77.60 @ hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "On sample 4 / 15 (hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22})\n",
      "On sample 5 / 15 (hyperparams = {'n_estimators': 305, 'max_depth': 7, 'min_samples_split': 63, 'min_samples_leaf': 4})\n",
      "On sample 6 / 15 (hyperparams = {'n_estimators': 443, 'max_depth': 4, 'min_samples_split': 24, 'min_samples_leaf': 5})\n",
      "On sample 7 / 15 (hyperparams = {'n_estimators': 253, 'max_depth': 6, 'min_samples_split': 59, 'min_samples_leaf': 25})\n",
      "On sample 8 / 15 (hyperparams = {'n_estimators': 183, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 44})\n",
      "On sample 9 / 15 (hyperparams = {'n_estimators': 385, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 13})\n",
      "On sample 10 / 15 (hyperparams = {'n_estimators': 498, 'max_depth': 4, 'min_samples_split': 62, 'min_samples_leaf': 27})\n",
      "On sample 11 / 15 (hyperparams = {'n_estimators': 194, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 17})\n",
      "On sample 12 / 15 (hyperparams = {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 46})\n",
      "New Best Score: 78.15 @ hyperparams = {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 46}\n",
      "On sample 13 / 15 (hyperparams = {'n_estimators': 121, 'max_depth': 8, 'min_samples_split': 49, 'min_samples_leaf': 42})\n",
      "On sample 14 / 15 (hyperparams = {'n_estimators': 287, 'max_depth': 4, 'min_samples_split': 74, 'min_samples_leaf': 19})\n",
      "On sample 15 / 15 (hyperparams = {'n_estimators': 440, 'max_depth': 6, 'min_samples_split': 32, 'min_samples_leaf': 16})\n",
      "Final results for model RF on target los_7 with representation data\n",
      "(0.7606418270608, 0.19693435584132754, 0.9233820459290187, 0.0)\n",
      "Running model RF on target los_3 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8})\n",
      "New Best Score: 70.67 @ hyperparams = {'n_estimators': 87, 'max_depth': 3, 'min_samples_split': 15, 'min_samples_leaf': 8}\n",
      "On sample 2 / 15 (hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4})\n",
      "New Best Score: 71.38 @ hyperparams = {'n_estimators': 285, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 4}\n",
      "On sample 3 / 15 (hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7})\n",
      "New Best Score: 72.28 @ hyperparams = {'n_estimators': 446, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "On sample 4 / 15 (hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22})\n",
      "New Best Score: 72.85 @ hyperparams = {'n_estimators': 122, 'max_depth': 8, 'min_samples_split': 65, 'min_samples_leaf': 22}\n",
      "On sample 5 / 15 (hyperparams = {'n_estimators': 305, 'max_depth': 7, 'min_samples_split': 63, 'min_samples_leaf': 4})\n",
      "On sample 6 / 15 (hyperparams = {'n_estimators': 443, 'max_depth': 4, 'min_samples_split': 24, 'min_samples_leaf': 5})\n",
      "On sample 7 / 15 (hyperparams = {'n_estimators': 253, 'max_depth': 6, 'min_samples_split': 59, 'min_samples_leaf': 25})\n",
      "On sample 8 / 15 (hyperparams = {'n_estimators': 183, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 44})\n",
      "On sample 9 / 15 (hyperparams = {'n_estimators': 385, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 13})\n",
      "On sample 10 / 15 (hyperparams = {'n_estimators': 498, 'max_depth': 4, 'min_samples_split': 62, 'min_samples_leaf': 27})\n",
      "On sample 11 / 15 (hyperparams = {'n_estimators': 194, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 17})\n",
      "On sample 12 / 15 (hyperparams = {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 46})\n",
      "On sample 13 / 15 (hyperparams = {'n_estimators': 121, 'max_depth': 8, 'min_samples_split': 49, 'min_samples_leaf': 42})\n",
      "On sample 14 / 15 (hyperparams = {'n_estimators': 287, 'max_depth': 4, 'min_samples_split': 74, 'min_samples_leaf': 19})\n",
      "On sample 15 / 15 (hyperparams = {'n_estimators': 440, 'max_depth': 6, 'min_samples_split': 32, 'min_samples_leaf': 16})\n",
      "Final results for model RF on target los_3 with representation data\n",
      "(0.7302392496273824, 0.6766193104195145, 0.6958246346555323, 0.591533501541912)\n",
      "Running model LR on target los_7 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'C': 0.001383611303681924, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "New Best Score: 68.09 @ hyperparams = {'C': 0.001383611303681924, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500}\n",
      "On sample 2 / 15 (hyperparams = {'C': 1.3047026700306064, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 3 / 15 (hyperparams = {'C': 0.003491839757169992, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 4 / 15 (hyperparams = {'C': 48.783036208459954, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 5 / 15 (hyperparams = {'C': 0.05459762073728651, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 6 / 15 (hyperparams = {'C': 1.081193410945589, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 7 / 15 (hyperparams = {'C': 0.0012201371230349724, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 8 / 15 (hyperparams = {'C': 0.3429679403524682, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 9 / 15 (hyperparams = {'C': 44.77804273778909, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 10 / 15 (hyperparams = {'C': 0.11951096159304532, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 11 / 15 (hyperparams = {'C': 433.47464177487655, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 12 / 15 (hyperparams = {'C': 1.7295128471299193, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 13 / 15 (hyperparams = {'C': 1.6777315525707752, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 14 / 15 (hyperparams = {'C': 0.059893230394471704, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 15 / 15 (hyperparams = {'C': 0.032340816118081595, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 100})\n",
      "Final results for model LR on target los_7 with representation data\n",
      "(0.672736211074018, 0.13902215447024618, 0.9179540709812108, 0.029629629629629627)\n",
      "Running model LR on target los_3 with representation data\n",
      "On sample 1 / 15 (hyperparams = {'C': 0.001383611303681924, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "New Best Score: 67.18 @ hyperparams = {'C': 0.001383611303681924, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500}\n",
      "On sample 2 / 15 (hyperparams = {'C': 1.3047026700306064, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 3 / 15 (hyperparams = {'C': 0.003491839757169992, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 4 / 15 (hyperparams = {'C': 48.783036208459954, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 5 / 15 (hyperparams = {'C': 0.05459762073728651, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 6 / 15 (hyperparams = {'C': 1.081193410945589, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 7 / 15 (hyperparams = {'C': 0.0012201371230349724, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 8 / 15 (hyperparams = {'C': 0.3429679403524682, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 9 / 15 (hyperparams = {'C': 44.77804273778909, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 10 / 15 (hyperparams = {'C': 0.11951096159304532, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 11 / 15 (hyperparams = {'C': 433.47464177487655, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "New Best Score: 67.25 @ hyperparams = {'C': 433.47464177487655, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500}\n",
      "On sample 12 / 15 (hyperparams = {'C': 1.7295128471299193, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 500})\n",
      "On sample 13 / 15 (hyperparams = {'C': 1.6777315525707752, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 14 / 15 (hyperparams = {'C': 0.059893230394471704, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 500})\n",
      "New Best Score: 68.87 @ hyperparams = {'C': 0.059893230394471704, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 500}\n",
      "On sample 15 / 15 (hyperparams = {'C': 0.032340816118081595, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 100})\n",
      "New Best Score: 69.22 @ hyperparams = {'C': 0.032340816118081595, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 100}\n",
      "Final results for model LR on target los_3 with representation data\n",
      "(0.6999474339003102, 0.6382947152889347, 0.6680584551148225, 0.5558659217877094)\n"
     ]
    }
   ],
   "source": [
    "for model_name, model, hyperparams_list in [\n",
    "    ('RF', RandomForestClassifier, RF_hyperparams_list), ('LR', LogisticRegression, LR_hyperparams_list)\n",
    "]:\n",
    "    if model_name not in results: \n",
    "        results[model_name] = {}\n",
    "    for t in ['los_7', 'los_3']:\n",
    "        if t not in results[model_name]: \n",
    "            results[model_name][t] = {}\n",
    "        \n",
    "        if \"data\" in results[model_name][t]:\n",
    "            print(\"Finished model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "            if RERUN: \n",
    "                h = results[model_name][t][\"data\"][1]\n",
    "                results[model_name][t][\"data\"] = run_only_final(model, h, data_flat_train, data_flat_dev, data_flat_test, t)\n",
    "\n",
    "                print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "                print(results[model_name][t][\"data\"][2:])\n",
    "\n",
    "                with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)\n",
    "            continue\n",
    "\n",
    "        print(\"Running model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        results[model_name][t][\"data\"] = run_basic(\n",
    "            model, hyperparams_list, data_flat_train, data_flat_dev, data_flat_test, t\n",
    "        )\n",
    "        print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        print(results[model_name][t][\"data\"][2:])  # auc, auprc, acc, F1\n",
    "        # with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model GRU-D on target los_3 with representation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-7d35c2d9607d>:22: RuntimeWarning: Mean of empty slice\n",
      "  X_mean = np.nanmean(tensor, axis=0, keepdims=True).transpose([0, 2, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 1 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 91, 'learning_rate': 0.056620731990215806, 'num_epochs': 30, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.09260225260560727, 'seed': 5121})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (rl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (hl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=91, bias=True)\n",
      "  (fc): Linear(in_features=91, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.82396394, valid_loss: 0.80602096, time: [5.03], best model: 1\n",
      "Epoch: 1, train_loss: 0.72999637, valid_loss: 0.74542825, time: [5.03], best model: 1\n",
      "Epoch: 2, train_loss: 0.71213195, valid_loss: 0.73072214, time: [4.97], best model: 1\n",
      "Epoch: 3, train_loss: 0.69552793, valid_loss: 0.70976244, time: [4.96], best model: 1\n",
      "Epoch: 4, train_loss: 0.67744129, valid_loss: 0.70863847, time: [4.98], best model: 1\n",
      "Epoch: 5, train_loss: 0.67096193, valid_loss: 0.70370813, time: [4.99], best model: 1\n",
      "Epoch: 6, train_loss: 0.65337986, valid_loss: 0.68691525, time: [4.97], best model: 1\n",
      "Epoch: 7, train_loss: 0.64732846, valid_loss: 0.67314885, time: [5.06], best model: 1\n",
      "Epoch: 8, train_loss: 0.63356387, valid_loss: 0.67187332, time: [4.99], best model: 1\n",
      "Epoch: 9, train_loss: 0.62175589, valid_loss: 0.66627457, time: [4.94], best model: 1\n",
      "Epoch: 10, train_loss: 0.61664995, valid_loss: 0.66652182, time: [4.96], best model: 0\n",
      "Epoch: 11, train_loss: 0.60616206, valid_loss: 0.6600186, time: [4.96], best model: 1\n",
      "Epoch: 12, train_loss: 0.6065957, valid_loss: 0.6619072, time: [4.95], best model: 0\n",
      "Epoch: 13, train_loss: 0.59690443, valid_loss: 0.66238817, time: [4.96], best model: 0\n",
      "Epoch: 14, train_loss: 0.59177522, valid_loss: 0.65568536, time: [4.98], best model: 1\n",
      "Epoch: 15, train_loss: 0.58613237, valid_loss: 0.65909984, time: [4.98], best model: 0\n",
      "Epoch: 16, train_loss: 0.58116945, valid_loss: 0.65933447, time: [5.], best model: 0\n",
      "Epoch: 17, train_loss: 0.57800196, valid_loss: 0.6592801, time: [4.95], best model: 0\n",
      "Epoch: 18, train_loss: 0.57142736, valid_loss: 0.66037071, time: [4.98], best model: 0\n",
      "Early Stopped at Epoch: 19\n",
      "New Best Score: 69.86 @ hyperparams = {'cell_size': 58, 'hidden_size': 91, 'learning_rate': 0.056620731990215806, 'num_epochs': 30, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.09260225260560727, 'seed': 5121}\n",
      "On sample 2 / 15 (hyperparams = {'cell_size': 65, 'hidden_size': 85, 'learning_rate': 0.04273078322899452, 'num_epochs': 132, 'patience': 5, 'batch_size': 64, 'early_stop_frac': 0.08228742134319564, 'seed': 5695})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (rl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (hl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=85, bias=True)\n",
      "  (fc): Linear(in_features=85, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.77418334, valid_loss: 0.76161353, time: [13.57], best model: 1\n",
      "Epoch: 1, train_loss: 0.70061811, valid_loss: 0.69310118, time: [13.51], best model: 1\n",
      "Epoch: 2, train_loss: 0.65541687, valid_loss: 0.65457433, time: [13.42], best model: 1\n",
      "Epoch: 3, train_loss: 0.634972, valid_loss: 0.63776652, time: [13.5], best model: 1\n",
      "Epoch: 4, train_loss: 0.62688058, valid_loss: 0.62587204, time: [13.48], best model: 1\n",
      "Epoch: 5, train_loss: 0.61836325, valid_loss: 0.62543011, time: [13.42], best model: 1\n",
      "Epoch: 6, train_loss: 0.61653379, valid_loss: 0.62969488, time: [13.49], best model: 0\n",
      "Epoch: 7, train_loss: 0.61228078, valid_loss: 0.63178062, time: [13.41], best model: 0\n",
      "Epoch: 8, train_loss: 0.61069838, valid_loss: 0.62985401, time: [13.47], best model: 0\n",
      "Epoch: 9, train_loss: 0.60365098, valid_loss: 0.63085874, time: [14.87], best model: 0\n",
      "Early Stopped at Epoch: 10\n",
      "New Best Score: 71.95 @ hyperparams = {'cell_size': 65, 'hidden_size': 85, 'learning_rate': 0.04273078322899452, 'num_epochs': 132, 'patience': 5, 'batch_size': 64, 'early_stop_frac': 0.08228742134319564, 'seed': 5695}\n",
      "On sample 3 / 15 (hyperparams = {'cell_size': 63, 'hidden_size': 93, 'learning_rate': 0.019698462366793658, 'num_epochs': 88, 'patience': 6, 'batch_size': 256, 'early_stop_frac': 0.12619299966519992, 'seed': 2473})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (rl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (hl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=93, bias=True)\n",
      "  (fc): Linear(in_features=93, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.79888762, valid_loss: 0.80009634, time: [5.37], best model: 1\n",
      "Epoch: 1, train_loss: 0.73095389, valid_loss: 0.75244435, time: [5.3], best model: 1\n",
      "Epoch: 2, train_loss: 0.70968059, valid_loss: 0.72807801, time: [5.23], best model: 1\n",
      "Epoch: 3, train_loss: 0.69334271, valid_loss: 0.71717185, time: [5.26], best model: 1\n",
      "Epoch: 4, train_loss: 0.67793555, valid_loss: 0.7040851, time: [5.16], best model: 1\n",
      "Epoch: 5, train_loss: 0.66800074, valid_loss: 0.69880643, time: [5.23], best model: 1\n",
      "Epoch: 6, train_loss: 0.65078227, valid_loss: 0.68940829, time: [5.25], best model: 1\n",
      "Epoch: 7, train_loss: 0.64187589, valid_loss: 0.67339452, time: [5.21], best model: 1\n",
      "Epoch: 8, train_loss: 0.63126828, valid_loss: 0.67067739, time: [5.25], best model: 1\n",
      "Epoch: 9, train_loss: 0.62093628, valid_loss: 0.66622577, time: [5.17], best model: 1\n",
      "Epoch: 10, train_loss: 0.61108419, valid_loss: 0.66530596, time: [5.23], best model: 1\n",
      "Epoch: 11, train_loss: 0.60699731, valid_loss: 0.65945174, time: [5.27], best model: 1\n",
      "Epoch: 12, train_loss: 0.60194564, valid_loss: 0.66675541, time: [5.22], best model: 0\n",
      "Epoch: 13, train_loss: 0.59779371, valid_loss: 0.66062954, time: [5.15], best model: 0\n",
      "Epoch: 14, train_loss: 0.5913535, valid_loss: 0.66154273, time: [5.15], best model: 0\n",
      "Epoch: 15, train_loss: 0.58956909, valid_loss: 0.649726, time: [5.15], best model: 1\n",
      "Epoch: 16, train_loss: 0.58350754, valid_loss: 0.65768392, time: [5.14], best model: 0\n",
      "Epoch: 17, train_loss: 0.57888192, valid_loss: 0.66296206, time: [5.07], best model: 0\n",
      "Epoch: 18, train_loss: 0.57441698, valid_loss: 0.67267749, time: [5.15], best model: 0\n",
      "Epoch: 19, train_loss: 0.57080767, valid_loss: 0.66609065, time: [5.08], best model: 0\n",
      "Epoch: 20, train_loss: 0.56473668, valid_loss: 0.67024465, time: [5.11], best model: 0\n",
      "Early Stopped at Epoch: 21\n",
      "On sample 4 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 85, 'learning_rate': 0.09896324058267929, 'num_epochs': 120, 'patience': 3, 'batch_size': 512, 'early_stop_frac': 0.14326285644110548, 'seed': 443})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (rl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (hl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=85, bias=True)\n",
      "  (fc): Linear(in_features=85, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.83197383, valid_loss: 0.83077458, time: [3.89], best model: 1\n",
      "Epoch: 1, train_loss: 0.74832882, valid_loss: 0.77083206, time: [3.92], best model: 1\n",
      "Epoch: 2, train_loss: 0.7363318, valid_loss: 0.76429367, time: [3.9], best model: 1\n",
      "Epoch: 3, train_loss: 0.71743495, valid_loss: 0.73541464, time: [3.84], best model: 1\n",
      "Epoch: 4, train_loss: 0.70496198, valid_loss: 0.73496934, time: [3.94], best model: 1\n",
      "Epoch: 5, train_loss: 0.69094733, valid_loss: 0.72260107, time: [3.87], best model: 1\n",
      "Epoch: 6, train_loss: 0.68082803, valid_loss: 0.71463667, time: [3.9], best model: 1\n",
      "Epoch: 7, train_loss: 0.67665713, valid_loss: 0.71904237, time: [3.87], best model: 0\n",
      "Epoch: 8, train_loss: 0.6586942, valid_loss: 0.70631729, time: [3.9], best model: 1\n",
      "Epoch: 9, train_loss: 0.6526242, valid_loss: 0.69237934, time: [3.85], best model: 1\n",
      "Epoch: 10, train_loss: 0.64049142, valid_loss: 0.69152648, time: [3.87], best model: 1\n",
      "Epoch: 11, train_loss: 0.63087402, valid_loss: 0.70984077, time: [3.83], best model: 0\n",
      "Epoch: 12, train_loss: 0.63411195, valid_loss: 0.69465392, time: [3.95], best model: 0\n",
      "Early Stopped at Epoch: 13\n",
      "On sample 5 / 15 (hyperparams = {'cell_size': 72, 'hidden_size': 70, 'learning_rate': 0.031701835710768775, 'num_epochs': 125, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.12112172427874425, 'seed': 4701})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (rl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (hl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=70, bias=True)\n",
      "  (fc): Linear(in_features=70, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.78844398, valid_loss: 0.79723017, time: [5.02], best model: 1\n",
      "Epoch: 1, train_loss: 0.74110633, valid_loss: 0.7775729, time: [4.95], best model: 1\n",
      "Epoch: 2, train_loss: 0.7215012, valid_loss: 0.75088882, time: [5.07], best model: 1\n",
      "Epoch: 3, train_loss: 0.68501322, valid_loss: 0.73317839, time: [4.96], best model: 1\n",
      "Epoch: 4, train_loss: 0.68081893, valid_loss: 0.71521732, time: [5.02], best model: 1\n",
      "Epoch: 5, train_loss: 0.66780365, valid_loss: 0.70324392, time: [4.78], best model: 1\n",
      "Epoch: 6, train_loss: 0.65321216, valid_loss: 0.69432978, time: [4.81], best model: 1\n",
      "Epoch: 7, train_loss: 0.64568663, valid_loss: 0.68415237, time: [4.88], best model: 1\n",
      "Epoch: 8, train_loss: 0.63928718, valid_loss: 0.678883, time: [4.78], best model: 1\n",
      "Epoch: 9, train_loss: 0.63622184, valid_loss: 0.67791601, time: [4.78], best model: 1\n",
      "Epoch: 10, train_loss: 0.62579004, valid_loss: 0.66618307, time: [4.78], best model: 1\n",
      "Epoch: 11, train_loss: 0.62060299, valid_loss: 0.66238049, time: [4.82], best model: 1\n",
      "Epoch: 12, train_loss: 0.61648038, valid_loss: 0.66860995, time: [4.8], best model: 0\n",
      "Epoch: 13, train_loss: 0.61030438, valid_loss: 0.66210094, time: [4.79], best model: 1\n",
      "Epoch: 14, train_loss: 0.60444996, valid_loss: 0.65960807, time: [4.81], best model: 1\n",
      "Epoch: 15, train_loss: 0.60048843, valid_loss: 0.6542857, time: [4.83], best model: 1\n",
      "Epoch: 16, train_loss: 0.59728187, valid_loss: 0.65083427, time: [4.78], best model: 1\n",
      "Epoch: 17, train_loss: 0.59756322, valid_loss: 0.64867649, time: [4.81], best model: 1\n",
      "Epoch: 18, train_loss: 0.59147744, valid_loss: 0.648151, time: [4.78], best model: 1\n",
      "Epoch: 19, train_loss: 0.58727197, valid_loss: 0.65466469, time: [4.81], best model: 0\n",
      "Epoch: 20, train_loss: 0.58279948, valid_loss: 0.65434787, time: [4.8], best model: 0\n",
      "Epoch: 21, train_loss: 0.58010945, valid_loss: 0.66051189, time: [4.79], best model: 0\n",
      "Epoch: 22, train_loss: 0.5723122, valid_loss: 0.66239079, time: [4.79], best model: 0\n",
      "Early Stopped at Epoch: 23\n",
      "On sample 6 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 72, 'learning_rate': 0.030786881587000436, 'num_epochs': 98, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.10174307637214693, 'seed': 7244})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (rl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (hl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=72, bias=True)\n",
      "  (fc): Linear(in_features=72, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.81972201, valid_loss: 0.79908187, time: [3.82], best model: 1\n",
      "Epoch: 1, train_loss: 0.75372847, valid_loss: 0.75536208, time: [3.79], best model: 1\n",
      "Epoch: 2, train_loss: 0.73368658, valid_loss: 0.74324483, time: [3.8], best model: 1\n",
      "Epoch: 3, train_loss: 0.7207929, valid_loss: 0.72928337, time: [3.78], best model: 1\n",
      "Epoch: 4, train_loss: 0.70390589, valid_loss: 0.71355813, time: [3.77], best model: 1\n",
      "Epoch: 5, train_loss: 0.68917268, valid_loss: 0.6989214, time: [3.75], best model: 1\n",
      "Epoch: 6, train_loss: 0.67431509, valid_loss: 0.69920119, time: [3.83], best model: 0\n",
      "Epoch: 7, train_loss: 0.67442078, valid_loss: 0.68919721, time: [3.79], best model: 1\n",
      "Epoch: 8, train_loss: 0.66395141, valid_loss: 0.67295186, time: [3.85], best model: 1\n",
      "Epoch: 9, train_loss: 0.65086102, valid_loss: 0.6775983, time: [3.83], best model: 0\n",
      "Epoch: 10, train_loss: 0.64904923, valid_loss: 0.67259802, time: [3.83], best model: 1\n",
      "Epoch: 11, train_loss: 0.63950618, valid_loss: 0.66661854, time: [3.76], best model: 1\n",
      "Epoch: 12, train_loss: 0.63368988, valid_loss: 0.65780455, time: [3.77], best model: 1\n",
      "Epoch: 13, train_loss: 0.62779834, valid_loss: 0.65576468, time: [3.83], best model: 1\n",
      "Epoch: 14, train_loss: 0.61896607, valid_loss: 0.65739954, time: [3.81], best model: 0\n",
      "Epoch: 15, train_loss: 0.6232826, valid_loss: 0.65643607, time: [3.82], best model: 0\n",
      "Epoch: 16, train_loss: 0.61066733, valid_loss: 0.66197277, time: [3.78], best model: 0\n",
      "Early Stopped at Epoch: 17\n",
      "On sample 7 / 15 (hyperparams = {'cell_size': 68, 'hidden_size': 68, 'learning_rate': 0.013619331759609964, 'num_epochs': 111, 'patience': 3, 'batch_size': 64, 'early_stop_frac': 0.13844715976978939, 'seed': 4659})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (rl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (hl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=68, bias=True)\n",
      "  (fc): Linear(in_features=68, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.78252062, valid_loss: 0.74880493, time: [12.78], best model: 1\n",
      "Epoch: 1, train_loss: 0.69836785, valid_loss: 0.67871331, time: [12.73], best model: 1\n",
      "Epoch: 2, train_loss: 0.66229411, valid_loss: 0.65279914, time: [12.7], best model: 1\n",
      "Epoch: 3, train_loss: 0.64486565, valid_loss: 0.62829508, time: [12.66], best model: 1\n",
      "Epoch: 4, train_loss: 0.63261163, valid_loss: 0.62537157, time: [12.72], best model: 1\n",
      "Epoch: 5, train_loss: 0.62221856, valid_loss: 0.61779731, time: [12.73], best model: 1\n",
      "Epoch: 6, train_loss: 0.61860535, valid_loss: 0.6252061, time: [12.75], best model: 0\n",
      "Epoch: 7, train_loss: 0.61351529, valid_loss: 0.62496697, time: [12.76], best model: 0\n",
      "Early Stopped at Epoch: 8\n",
      "On sample 8 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 71, 'learning_rate': 0.02017270377811787, 'num_epochs': 58, 'patience': 3, 'batch_size': 128, 'early_stop_frac': 0.10536338408272322, 'seed': 1226})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (rl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (hl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=71, bias=True)\n",
      "  (fc): Linear(in_features=71, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.79067328, valid_loss: 0.79714575, time: [8.76], best model: 1\n",
      "Epoch: 1, train_loss: 0.72416661, valid_loss: 0.73592605, time: [8.62], best model: 1\n",
      "Epoch: 2, train_loss: 0.70004129, valid_loss: 0.70056765, time: [8.6], best model: 1\n",
      "Epoch: 3, train_loss: 0.67296274, valid_loss: 0.67317891, time: [8.6], best model: 1\n",
      "Epoch: 4, train_loss: 0.64690869, valid_loss: 0.6695526, time: [8.59], best model: 1\n",
      "Epoch: 5, train_loss: 0.6369447, valid_loss: 0.65692686, time: [8.55], best model: 1\n",
      "Epoch: 6, train_loss: 0.62822254, valid_loss: 0.64025201, time: [8.19], best model: 1\n",
      "Epoch: 7, train_loss: 0.62138628, valid_loss: 0.64134066, time: [8.2], best model: 0\n",
      "Epoch: 8, train_loss: 0.61991198, valid_loss: 0.63728743, time: [8.43], best model: 1\n",
      "Epoch: 9, train_loss: 0.61166199, valid_loss: 0.63419388, time: [8.18], best model: 1\n",
      "Epoch: 10, train_loss: 0.61317607, valid_loss: 0.63301334, time: [8.4], best model: 1\n",
      "Epoch: 11, train_loss: 0.60629077, valid_loss: 0.63571767, time: [8.23], best model: 0\n",
      "Epoch: 12, train_loss: 0.6053324, valid_loss: 0.63547209, time: [8.25], best model: 0\n",
      "Early Stopped at Epoch: 13\n",
      "On sample 9 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 69, 'learning_rate': 0.05142897677179076, 'num_epochs': 47, 'patience': 4, 'batch_size': 256, 'early_stop_frac': 0.10736424951768658, 'seed': 6792})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (rl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (hl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=69, bias=True)\n",
      "  (fc): Linear(in_features=69, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.80800543, valid_loss: 0.79360488, time: [4.95], best model: 1\n",
      "Epoch: 1, train_loss: 0.73582643, valid_loss: 0.73318955, time: [4.93], best model: 1\n",
      "Epoch: 2, train_loss: 0.72214008, valid_loss: 0.70775946, time: [5.03], best model: 1\n",
      "Epoch: 3, train_loss: 0.70365768, valid_loss: 0.69368034, time: [4.9], best model: 1\n",
      "Epoch: 4, train_loss: 0.68265566, valid_loss: 0.68402928, time: [4.96], best model: 1\n",
      "Epoch: 5, train_loss: 0.66100015, valid_loss: 0.6736047, time: [4.96], best model: 1\n",
      "Epoch: 6, train_loss: 0.66072313, valid_loss: 0.66828498, time: [4.99], best model: 1\n",
      "Epoch: 7, train_loss: 0.64915815, valid_loss: 0.65608031, time: [4.9], best model: 1\n",
      "Epoch: 8, train_loss: 0.63533158, valid_loss: 0.65171446, time: [4.9], best model: 1\n",
      "Epoch: 9, train_loss: 0.63049178, valid_loss: 0.64841711, time: [4.98], best model: 1\n",
      "Epoch: 10, train_loss: 0.62373194, valid_loss: 0.64533319, time: [4.96], best model: 1\n",
      "Epoch: 11, train_loss: 0.6129326, valid_loss: 0.65109667, time: [4.97], best model: 0\n",
      "Epoch: 12, train_loss: 0.60818153, valid_loss: 0.64165063, time: [4.92], best model: 1\n",
      "Epoch: 13, train_loss: 0.60594335, valid_loss: 0.64689643, time: [4.92], best model: 0\n",
      "Epoch: 14, train_loss: 0.60746101, valid_loss: 0.64313329, time: [4.9], best model: 0\n",
      "Epoch: 15, train_loss: 0.60014718, valid_loss: 0.64363841, time: [4.89], best model: 0\n",
      "Early Stopped at Epoch: 16\n",
      "On sample 10 / 15 (hyperparams = {'cell_size': 57, 'hidden_size': 75, 'learning_rate': 0.05857651292235553, 'num_epochs': 41, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08939332949520416, 'seed': 9043})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.7690088, valid_loss: 0.78193363, time: [13.73], best model: 1\n",
      "Epoch: 1, train_loss: 0.69797016, valid_loss: 0.71190432, time: [13.52], best model: 1\n",
      "Epoch: 2, train_loss: 0.65095456, valid_loss: 0.67021314, time: [13.41], best model: 1\n",
      "Epoch: 3, train_loss: 0.63476441, valid_loss: 0.6531424, time: [13.43], best model: 1\n",
      "Epoch: 4, train_loss: 0.62450678, valid_loss: 0.64394, time: [13.47], best model: 1\n",
      "Epoch: 5, train_loss: 0.61884, valid_loss: 0.63818212, time: [13.5], best model: 1\n",
      "Epoch: 6, train_loss: 0.61557443, valid_loss: 0.63603678, time: [13.55], best model: 1\n",
      "Epoch: 7, train_loss: 0.61232892, valid_loss: 0.63995586, time: [13.45], best model: 0\n",
      "Epoch: 8, train_loss: 0.60862533, valid_loss: 0.64008075, time: [13.78], best model: 0\n",
      "Epoch: 9, train_loss: 0.60920997, valid_loss: 0.6421839, time: [13.59], best model: 0\n",
      "Early Stopped at Epoch: 10\n",
      "New Best Score: 72.03 @ hyperparams = {'cell_size': 57, 'hidden_size': 75, 'learning_rate': 0.05857651292235553, 'num_epochs': 41, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08939332949520416, 'seed': 9043}\n",
      "On sample 11 / 15 (hyperparams = {'cell_size': 52, 'hidden_size': 76, 'learning_rate': 0.024183516701270405, 'num_epochs': 91, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.1426546750491463, 'seed': 1244})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (rl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (hl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=76, bias=True)\n",
      "  (fc): Linear(in_features=76, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.81495939, valid_loss: 0.79676642, time: [4.87], best model: 1\n",
      "Epoch: 1, train_loss: 0.75234107, valid_loss: 0.72409269, time: [4.89], best model: 1\n",
      "Epoch: 2, train_loss: 0.72105449, valid_loss: 0.7109766, time: [4.78], best model: 1\n",
      "Epoch: 3, train_loss: 0.68904182, valid_loss: 0.69751814, time: [4.75], best model: 1\n",
      "Epoch: 4, train_loss: 0.6808174, valid_loss: 0.69245359, time: [4.74], best model: 1\n",
      "Epoch: 5, train_loss: 0.66362013, valid_loss: 0.68140459, time: [4.77], best model: 1\n",
      "Epoch: 6, train_loss: 0.6537103, valid_loss: 0.66674273, time: [4.78], best model: 1\n",
      "Epoch: 7, train_loss: 0.64335537, valid_loss: 0.66565336, time: [4.76], best model: 1\n",
      "Epoch: 8, train_loss: 0.63716085, valid_loss: 0.65367324, time: [4.73], best model: 1\n",
      "Epoch: 9, train_loss: 0.63552986, valid_loss: 0.65576274, time: [4.72], best model: 0\n",
      "Epoch: 10, train_loss: 0.62184198, valid_loss: 0.65739972, time: [4.84], best model: 0\n",
      "Epoch: 11, train_loss: 0.6189693, valid_loss: 0.65021501, time: [4.76], best model: 1\n",
      "Epoch: 12, train_loss: 0.61267614, valid_loss: 0.64895412, time: [4.89], best model: 1\n",
      "Epoch: 13, train_loss: 0.60430568, valid_loss: 0.64383732, time: [4.84], best model: 1\n",
      "Epoch: 14, train_loss: 0.60071795, valid_loss: 0.64515911, time: [4.76], best model: 0\n",
      "Epoch: 15, train_loss: 0.59188911, valid_loss: 0.6460106, time: [4.78], best model: 0\n",
      "Epoch: 16, train_loss: 0.59523753, valid_loss: 0.64622382, time: [4.77], best model: 0\n",
      "Epoch: 17, train_loss: 0.59016262, valid_loss: 0.64826659, time: [4.75], best model: 0\n",
      "Early Stopped at Epoch: 18\n",
      "On sample 12 / 15 (hyperparams = {'cell_size': 67, 'hidden_size': 84, 'learning_rate': 0.07874911709152625, 'num_epochs': 55, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.050554441943176515, 'seed': 9967})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (rl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (hl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=84, bias=True)\n",
      "  (fc): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.83481407, valid_loss: 0.8207614, time: [4.09], best model: 1\n",
      "Epoch: 1, train_loss: 0.76368806, valid_loss: 0.76378675, time: [4.06], best model: 1\n",
      "Epoch: 2, train_loss: 0.7429505, valid_loss: 0.74655397, time: [4.07], best model: 1\n",
      "Epoch: 3, train_loss: 0.72265742, valid_loss: 0.73809218, time: [4.1], best model: 1\n",
      "Epoch: 4, train_loss: 0.69511715, valid_loss: 0.71086059, time: [4.52], best model: 1\n",
      "Epoch: 5, train_loss: 0.68804802, valid_loss: 0.72068196, time: [4.13], best model: 0\n",
      "Epoch: 6, train_loss: 0.67855066, valid_loss: 0.69346625, time: [4.15], best model: 1\n",
      "Epoch: 7, train_loss: 0.66268656, valid_loss: 0.70105448, time: [4.2], best model: 0\n",
      "Epoch: 8, train_loss: 0.66700929, valid_loss: 0.68600113, time: [4.08], best model: 1\n",
      "Epoch: 9, train_loss: 0.65807626, valid_loss: 0.69787241, time: [4.07], best model: 0\n",
      "Epoch: 10, train_loss: 0.64431837, valid_loss: 0.68051043, time: [4.13], best model: 1\n",
      "Epoch: 11, train_loss: 0.63431223, valid_loss: 0.67307626, time: [4.2], best model: 1\n",
      "Epoch: 12, train_loss: 0.632753, valid_loss: 0.67555987, time: [4.14], best model: 0\n",
      "Epoch: 13, train_loss: 0.62853739, valid_loss: 0.68661074, time: [4.12], best model: 0\n",
      "Epoch: 14, train_loss: 0.62094098, valid_loss: 0.68237846, time: [4.12], best model: 0\n",
      "Early Stopped at Epoch: 15\n",
      "On sample 13 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 72, 'learning_rate': 0.0597308068795698, 'num_epochs': 49, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.12284984962121943, 'seed': 8417})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (rl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (hl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=72, bias=True)\n",
      "  (fc): Linear(in_features=72, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.81809889, valid_loss: 0.82147884, time: [3.77], best model: 1\n",
      "Epoch: 1, train_loss: 0.75446926, valid_loss: 0.78921795, time: [3.79], best model: 1\n",
      "Epoch: 2, train_loss: 0.72228616, valid_loss: 0.7703369, time: [3.71], best model: 1\n",
      "Epoch: 3, train_loss: 0.71439689, valid_loss: 0.75239754, time: [3.92], best model: 1\n",
      "Epoch: 4, train_loss: 0.71430506, valid_loss: 0.73778677, time: [3.89], best model: 1\n",
      "Epoch: 5, train_loss: 0.70385422, valid_loss: 0.73266697, time: [3.78], best model: 1\n",
      "Epoch: 6, train_loss: 0.68681254, valid_loss: 0.72512797, time: [3.68], best model: 1\n",
      "Epoch: 7, train_loss: 0.66182205, valid_loss: 0.70458937, time: [3.67], best model: 1\n",
      "Epoch: 8, train_loss: 0.65823494, valid_loss: 0.70901557, time: [3.75], best model: 0\n",
      "Epoch: 9, train_loss: 0.65789734, valid_loss: 0.69876548, time: [3.74], best model: 1\n",
      "Epoch: 10, train_loss: 0.64839915, valid_loss: 0.69548518, time: [3.75], best model: 1\n",
      "Epoch: 11, train_loss: 0.64205578, valid_loss: 0.69919355, time: [3.72], best model: 0\n",
      "Epoch: 12, train_loss: 0.62947886, valid_loss: 0.6931198, time: [3.73], best model: 1\n",
      "Epoch: 13, train_loss: 0.62941558, valid_loss: 0.68409491, time: [3.71], best model: 1\n",
      "Epoch: 14, train_loss: 0.62722547, valid_loss: 0.68841076, time: [3.71], best model: 0\n",
      "Epoch: 15, train_loss: 0.61104761, valid_loss: 0.68581731, time: [3.7], best model: 0\n",
      "Epoch: 16, train_loss: 0.61253841, valid_loss: 0.67947108, time: [3.73], best model: 1\n",
      "Epoch: 17, train_loss: 0.5994322, valid_loss: 0.67607927, time: [3.8], best model: 1\n",
      "Epoch: 18, train_loss: 0.59514959, valid_loss: 0.68126243, time: [3.62], best model: 0\n",
      "Epoch: 19, train_loss: 0.59242092, valid_loss: 0.68169914, time: [3.72], best model: 0\n",
      "Epoch: 20, train_loss: 0.58950199, valid_loss: 0.67226437, time: [3.7], best model: 1\n",
      "Epoch: 21, train_loss: 0.58187451, valid_loss: 0.67670836, time: [3.73], best model: 0\n",
      "Epoch: 22, train_loss: 0.57930272, valid_loss: 0.67346995, time: [3.75], best model: 0\n",
      "Epoch: 23, train_loss: 0.5795583, valid_loss: 0.67694868, time: [3.7], best model: 0\n",
      "Early Stopped at Epoch: 24\n",
      "On sample 14 / 15 (hyperparams = {'cell_size': 71, 'hidden_size': 71, 'learning_rate': 0.018782331070145576, 'num_epochs': 75, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08557536288902531, 'seed': 50})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (rl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (hl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=71, bias=True)\n",
      "  (fc): Linear(in_features=71, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.78626133, valid_loss: 0.75062172, time: [13.72], best model: 1\n",
      "Epoch: 1, train_loss: 0.69395485, valid_loss: 0.67274564, time: [13.7], best model: 1\n",
      "Epoch: 2, train_loss: 0.65999938, valid_loss: 0.64218037, time: [13.69], best model: 1\n",
      "Epoch: 3, train_loss: 0.63889852, valid_loss: 0.6283358, time: [13.59], best model: 1\n",
      "Epoch: 4, train_loss: 0.62757439, valid_loss: 0.61992623, time: [13.55], best model: 1\n",
      "Epoch: 5, train_loss: 0.62337101, valid_loss: 0.61685315, time: [13.57], best model: 1\n",
      "Epoch: 6, train_loss: 0.62124915, valid_loss: 0.61920945, time: [13.6], best model: 0\n",
      "Epoch: 7, train_loss: 0.61883673, valid_loss: 0.61595645, time: [13.65], best model: 1\n",
      "Epoch: 8, train_loss: 0.61220266, valid_loss: 0.6198906, time: [13.58], best model: 0\n",
      "Epoch: 9, train_loss: 0.61135641, valid_loss: 0.61700024, time: [13.64], best model: 0\n",
      "Epoch: 10, train_loss: 0.60648822, valid_loss: 0.62006471, time: [13.79], best model: 0\n",
      "Early Stopped at Epoch: 11\n",
      "On sample 15 / 15 (hyperparams = {'cell_size': 65, 'hidden_size': 75, 'learning_rate': 0.03874713686238469, 'num_epochs': 85, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.07129332368775368, 'seed': 297})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.76800587, valid_loss: 0.74563705, time: [14.14], best model: 1\n",
      "Epoch: 1, train_loss: 0.69762336, valid_loss: 0.66639308, time: [14.02], best model: 1\n",
      "Epoch: 2, train_loss: 0.65520298, valid_loss: 0.63813154, time: [13.96], best model: 1\n",
      "Epoch: 3, train_loss: 0.63728301, valid_loss: 0.62463605, time: [13.97], best model: 1\n",
      "Epoch: 4, train_loss: 0.62120006, valid_loss: 0.61807653, time: [13.87], best model: 1\n",
      "Epoch: 5, train_loss: 0.62304003, valid_loss: 0.61783101, time: [13.96], best model: 1\n",
      "Epoch: 6, train_loss: 0.61812325, valid_loss: 0.61257552, time: [13.93], best model: 1\n",
      "Epoch: 7, train_loss: 0.61525968, valid_loss: 0.61583644, time: [13.91], best model: 0\n",
      "Epoch: 8, train_loss: 0.61139852, valid_loss: 0.61452751, time: [13.88], best model: 0\n",
      "Epoch: 9, train_loss: 0.60555974, valid_loss: 0.61732703, time: [13.81], best model: 0\n",
      "Early Stopped at Epoch: 10\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.77053468, valid_loss: 0.74030921, time: [15.61], best model: 1\n",
      "Epoch: 1, train_loss: 0.68911592, valid_loss: 0.66430406, time: [15.52], best model: 1\n",
      "Epoch: 2, train_loss: 0.65997702, valid_loss: 0.63504045, time: [15.54], best model: 1\n",
      "Epoch: 3, train_loss: 0.63055538, valid_loss: 0.61921355, time: [16.05], best model: 1\n",
      "Epoch: 4, train_loss: 0.62688396, valid_loss: 0.61755943, time: [15.66], best model: 1\n",
      "Epoch: 5, train_loss: 0.61965303, valid_loss: 0.61353835, time: [15.45], best model: 1\n",
      "Epoch: 6, train_loss: 0.61882182, valid_loss: 0.61360404, time: [15.51], best model: 0\n",
      "Epoch: 7, train_loss: 0.61630894, valid_loss: 0.60821629, time: [15.45], best model: 1\n",
      "Epoch: 8, train_loss: 0.61331631, valid_loss: 0.61109128, time: [15.59], best model: 0\n",
      "Epoch: 9, train_loss: 0.60869856, valid_loss: 0.61111848, time: [16.24], best model: 0\n",
      "Epoch: 10, train_loss: 0.60640834, valid_loss: 0.61503135, time: [16.22], best model: 0\n",
      "Early Stopped at Epoch: 11\n",
      "Final results for model GRU-D on target los_3 with representation data\n",
      "0.7249252269890112 0.6815624266046457 0.6906672297297297 0.5881360697216755\n",
      "Running model GRU-D on target los_7 with representation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-7d35c2d9607d>:22: RuntimeWarning: Mean of empty slice\n",
      "  X_mean = np.nanmean(tensor, axis=0, keepdims=True).transpose([0, 2, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On sample 1 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 91, 'learning_rate': 0.056620731990215806, 'num_epochs': 30, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.09260225260560727, 'seed': 5121})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (rl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (hl): Linear(in_features=299, out_features=91, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=91, bias=True)\n",
      "  (fc): Linear(in_features=91, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.84016884, valid_loss: 0.85484075, time: [5.12], best model: 1\n",
      "Epoch: 1, train_loss: 0.75291766, valid_loss: 0.76328109, time: [5.04], best model: 1\n",
      "Epoch: 2, train_loss: 0.6862943, valid_loss: 0.69341692, time: [5.14], best model: 1\n",
      "Epoch: 3, train_loss: 0.62797443, valid_loss: 0.63955372, time: [5.03], best model: 1\n",
      "Epoch: 4, train_loss: 0.57583877, valid_loss: 0.59067813, time: [5.08], best model: 1\n",
      "Epoch: 5, train_loss: 0.56190504, valid_loss: 0.55272991, time: [5.06], best model: 1\n",
      "Epoch: 6, train_loss: 0.52783733, valid_loss: 0.55373518, time: [4.98], best model: 0\n",
      "Epoch: 7, train_loss: 0.52143252, valid_loss: 0.5140024, time: [4.99], best model: 1\n",
      "Epoch: 8, train_loss: 0.49074904, valid_loss: 0.50414014, time: [5.02], best model: 1\n",
      "Epoch: 9, train_loss: 0.48448715, valid_loss: 0.48518637, time: [5.02], best model: 1\n",
      "Epoch: 10, train_loss: 0.46296252, valid_loss: 0.47326486, time: [5.05], best model: 1\n",
      "Epoch: 11, train_loss: 0.44961309, valid_loss: 0.45768544, time: [5.09], best model: 1\n",
      "Epoch: 12, train_loss: 0.43630619, valid_loss: 0.4513082, time: [5.09], best model: 1\n",
      "Epoch: 13, train_loss: 0.43502779, valid_loss: 0.44027468, time: [5.], best model: 1\n",
      "Epoch: 14, train_loss: 0.42713388, valid_loss: 0.4462695, time: [5.06], best model: 0\n",
      "Epoch: 15, train_loss: 0.41746207, valid_loss: 0.42238197, time: [5.06], best model: 1\n",
      "Epoch: 16, train_loss: 0.41061447, valid_loss: 0.41755253, time: [5.01], best model: 1\n",
      "Epoch: 17, train_loss: 0.40616039, valid_loss: 0.40857296, time: [5.04], best model: 1\n",
      "Epoch: 18, train_loss: 0.38975923, valid_loss: 0.40856109, time: [5.03], best model: 1\n",
      "Epoch: 19, train_loss: 0.38538283, valid_loss: 0.39837853, time: [5.01], best model: 1\n",
      "Epoch: 20, train_loss: 0.37678395, valid_loss: 0.3975373, time: [5.06], best model: 1\n",
      "Epoch: 21, train_loss: 0.37302913, valid_loss: 0.39055129, time: [5.07], best model: 1\n",
      "Epoch: 22, train_loss: 0.36570407, valid_loss: 0.38236747, time: [5.], best model: 1\n",
      "Epoch: 23, train_loss: 0.35703032, valid_loss: 0.38074115, time: [5.04], best model: 1\n",
      "Epoch: 24, train_loss: 0.35711515, valid_loss: 0.38268568, time: [5.12], best model: 0\n",
      "Epoch: 25, train_loss: 0.35137177, valid_loss: 0.38584493, time: [5.03], best model: 0\n",
      "Epoch: 26, train_loss: 0.34345152, valid_loss: 0.38774041, time: [5.08], best model: 0\n",
      "Epoch: 27, train_loss: 0.34031868, valid_loss: 0.38194679, time: [5.14], best model: 0\n",
      "Early Stopped at Epoch: 28\n",
      "New Best Score: 64.79 @ hyperparams = {'cell_size': 58, 'hidden_size': 91, 'learning_rate': 0.056620731990215806, 'num_epochs': 30, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.09260225260560727, 'seed': 5121}\n",
      "On sample 2 / 15 (hyperparams = {'cell_size': 65, 'hidden_size': 85, 'learning_rate': 0.04273078322899452, 'num_epochs': 132, 'patience': 5, 'batch_size': 64, 'early_stop_frac': 0.08228742134319564, 'seed': 5695})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (rl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (hl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=85, bias=True)\n",
      "  (fc): Linear(in_features=85, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.7771623, valid_loss: 0.76971976, time: [13.82], best model: 1\n",
      "Epoch: 1, train_loss: 0.59105072, valid_loss: 0.59223321, time: [14.], best model: 1\n",
      "Epoch: 2, train_loss: 0.48444668, valid_loss: 0.49918537, time: [14.25], best model: 1\n",
      "Epoch: 3, train_loss: 0.43439681, valid_loss: 0.43789069, time: [13.79], best model: 1\n",
      "Epoch: 4, train_loss: 0.4002031, valid_loss: 0.41028512, time: [13.78], best model: 1\n",
      "Epoch: 5, train_loss: 0.38220514, valid_loss: 0.3914779, time: [13.64], best model: 1\n",
      "Epoch: 6, train_loss: 0.37350597, valid_loss: 0.38856134, time: [13.72], best model: 1\n",
      "Epoch: 7, train_loss: 0.37001619, valid_loss: 0.37965759, time: [13.62], best model: 1\n",
      "Epoch: 8, train_loss: 0.36811463, valid_loss: 0.38458249, time: [13.67], best model: 0\n",
      "Epoch: 9, train_loss: 0.3527909, valid_loss: 0.3775163, time: [13.75], best model: 1\n",
      "Epoch: 10, train_loss: 0.36096334, valid_loss: 0.38604066, time: [13.79], best model: 0\n",
      "Epoch: 11, train_loss: 0.35172008, valid_loss: 0.38132556, time: [13.8], best model: 0\n",
      "Epoch: 12, train_loss: 0.35319058, valid_loss: 0.38066686, time: [13.73], best model: 0\n",
      "Epoch: 13, train_loss: 0.3433204, valid_loss: 0.38443565, time: [13.67], best model: 0\n",
      "Early Stopped at Epoch: 14\n",
      "New Best Score: 70.49 @ hyperparams = {'cell_size': 65, 'hidden_size': 85, 'learning_rate': 0.04273078322899452, 'num_epochs': 132, 'patience': 5, 'batch_size': 64, 'early_stop_frac': 0.08228742134319564, 'seed': 5695}\n",
      "On sample 3 / 15 (hyperparams = {'cell_size': 63, 'hidden_size': 93, 'learning_rate': 0.019698462366793658, 'num_epochs': 88, 'patience': 6, 'batch_size': 256, 'early_stop_frac': 0.12619299966519992, 'seed': 2473})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (rl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (hl): Linear(in_features=301, out_features=93, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=93, bias=True)\n",
      "  (fc): Linear(in_features=93, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.84550101, valid_loss: 0.84604859, time: [4.89], best model: 1\n",
      "Epoch: 1, train_loss: 0.75972112, valid_loss: 0.76773312, time: [4.87], best model: 1\n",
      "Epoch: 2, train_loss: 0.70154505, valid_loss: 0.71131482, time: [4.88], best model: 1\n",
      "Epoch: 3, train_loss: 0.64267262, valid_loss: 0.65434968, time: [4.84], best model: 1\n",
      "Epoch: 4, train_loss: 0.59591936, valid_loss: 0.59142879, time: [4.84], best model: 1\n",
      "Epoch: 5, train_loss: 0.54648185, valid_loss: 0.56674342, time: [4.84], best model: 1\n",
      "Epoch: 6, train_loss: 0.53562238, valid_loss: 0.5414424, time: [4.82], best model: 1\n",
      "Epoch: 7, train_loss: 0.51691249, valid_loss: 0.52412548, time: [4.88], best model: 1\n",
      "Epoch: 8, train_loss: 0.49018408, valid_loss: 0.5044724, time: [4.85], best model: 1\n",
      "Epoch: 9, train_loss: 0.47154366, valid_loss: 0.48460234, time: [4.82], best model: 1\n",
      "Epoch: 10, train_loss: 0.4736205, valid_loss: 0.482057, time: [4.84], best model: 1\n",
      "Epoch: 11, train_loss: 0.45865805, valid_loss: 0.46353136, time: [4.82], best model: 1\n",
      "Epoch: 12, train_loss: 0.44304035, valid_loss: 0.45770354, time: [4.85], best model: 1\n",
      "Epoch: 13, train_loss: 0.43644487, valid_loss: 0.45046361, time: [4.83], best model: 1\n",
      "Epoch: 14, train_loss: 0.43144293, valid_loss: 0.43606631, time: [5.16], best model: 1\n",
      "Epoch: 15, train_loss: 0.41304873, valid_loss: 0.43206637, time: [5.2], best model: 1\n",
      "Epoch: 16, train_loss: 0.41032875, valid_loss: 0.42961208, time: [5.12], best model: 1\n",
      "Epoch: 17, train_loss: 0.40196429, valid_loss: 0.4225415, time: [5.1], best model: 1\n",
      "Epoch: 18, train_loss: 0.39416885, valid_loss: 0.41259177, time: [5.09], best model: 1\n",
      "Epoch: 19, train_loss: 0.39230527, valid_loss: 0.40936815, time: [5.07], best model: 1\n",
      "Epoch: 20, train_loss: 0.37759828, valid_loss: 0.40847554, time: [5.11], best model: 1\n",
      "Epoch: 21, train_loss: 0.36944938, valid_loss: 0.39990465, time: [5.15], best model: 1\n",
      "Epoch: 22, train_loss: 0.36424155, valid_loss: 0.39974166, time: [5.06], best model: 1\n",
      "Epoch: 23, train_loss: 0.35428044, valid_loss: 0.39534, time: [5.03], best model: 1\n",
      "Epoch: 24, train_loss: 0.3514253, valid_loss: 0.39444204, time: [5.12], best model: 1\n",
      "Epoch: 25, train_loss: 0.3522039, valid_loss: 0.38951031, time: [5.19], best model: 1\n",
      "Epoch: 26, train_loss: 0.34302708, valid_loss: 0.3920618, time: [5.15], best model: 0\n",
      "Epoch: 27, train_loss: 0.33740341, valid_loss: 0.39315736, time: [5.14], best model: 0\n",
      "Epoch: 28, train_loss: 0.33252666, valid_loss: 0.3960749, time: [5.05], best model: 0\n",
      "Epoch: 29, train_loss: 0.32846424, valid_loss: 0.39178443, time: [5.15], best model: 0\n",
      "Epoch: 30, train_loss: 0.32466868, valid_loss: 0.39233439, time: [5.15], best model: 0\n",
      "Early Stopped at Epoch: 31\n",
      "On sample 4 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 85, 'learning_rate': 0.09896324058267929, 'num_epochs': 120, 'patience': 3, 'batch_size': 512, 'early_stop_frac': 0.14326285644110548, 'seed': 443})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (rl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (hl): Linear(in_features=293, out_features=85, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=85, bias=True)\n",
      "  (fc): Linear(in_features=85, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.88063492, valid_loss: 0.86523022, time: [3.92], best model: 1\n",
      "Epoch: 1, train_loss: 0.82399702, valid_loss: 0.8213785, time: [3.97], best model: 1\n",
      "Epoch: 2, train_loss: 0.78229141, valid_loss: 0.7903112, time: [4.07], best model: 1\n",
      "Epoch: 3, train_loss: 0.75488172, valid_loss: 0.75676087, time: [3.86], best model: 1\n",
      "Epoch: 4, train_loss: 0.72770071, valid_loss: 0.73439319, time: [3.89], best model: 1\n",
      "Epoch: 5, train_loss: 0.69871214, valid_loss: 0.7042017, time: [3.94], best model: 1\n",
      "Epoch: 6, train_loss: 0.67242255, valid_loss: 0.66423804, time: [3.98], best model: 1\n",
      "Epoch: 7, train_loss: 0.63799592, valid_loss: 0.63799633, time: [4.04], best model: 1\n",
      "Epoch: 8, train_loss: 0.59629018, valid_loss: 0.61682531, time: [3.9], best model: 1\n",
      "Epoch: 9, train_loss: 0.58501094, valid_loss: 0.57917649, time: [3.91], best model: 1\n",
      "Epoch: 10, train_loss: 0.55883387, valid_loss: 0.56113812, time: [3.96], best model: 1\n",
      "Epoch: 11, train_loss: 0.54212557, valid_loss: 0.55927835, time: [3.95], best model: 1\n",
      "Epoch: 12, train_loss: 0.52520016, valid_loss: 0.53569596, time: [3.83], best model: 1\n",
      "Epoch: 13, train_loss: 0.52115086, valid_loss: 0.54543042, time: [3.95], best model: 0\n",
      "Epoch: 14, train_loss: 0.519107, valid_loss: 0.52437561, time: [3.96], best model: 1\n",
      "Epoch: 15, train_loss: 0.50109403, valid_loss: 0.52879981, time: [3.89], best model: 0\n",
      "Epoch: 16, train_loss: 0.49823073, valid_loss: 0.54209123, time: [4.01], best model: 0\n",
      "Epoch: 17, train_loss: 0.501447, valid_loss: 0.50527862, time: [3.93], best model: 1\n",
      "Epoch: 18, train_loss: 0.4837013, valid_loss: 0.49012136, time: [3.92], best model: 1\n",
      "Epoch: 19, train_loss: 0.4855801, valid_loss: 0.50813099, time: [3.91], best model: 0\n",
      "Epoch: 20, train_loss: 0.46531064, valid_loss: 0.46471204, time: [3.94], best model: 1\n",
      "Epoch: 21, train_loss: 0.45151247, valid_loss: 0.46794292, time: [4.18], best model: 0\n",
      "Epoch: 22, train_loss: 0.4465628, valid_loss: 0.47085122, time: [3.9], best model: 0\n",
      "Early Stopped at Epoch: 23\n",
      "On sample 5 / 15 (hyperparams = {'cell_size': 72, 'hidden_size': 70, 'learning_rate': 0.031701835710768775, 'num_epochs': 125, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.12112172427874425, 'seed': 4701})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (rl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (hl): Linear(in_features=278, out_features=70, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=70, bias=True)\n",
      "  (fc): Linear(in_features=70, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.85019409, valid_loss: 0.85670498, time: [4.99], best model: 1\n",
      "Epoch: 1, train_loss: 0.76576554, valid_loss: 0.78109581, time: [4.98], best model: 1\n",
      "Epoch: 2, train_loss: 0.72920254, valid_loss: 0.72044232, time: [5.04], best model: 1\n",
      "Epoch: 3, train_loss: 0.66262831, valid_loss: 0.67343608, time: [4.96], best model: 1\n",
      "Epoch: 4, train_loss: 0.62377997, valid_loss: 0.63759894, time: [5.03], best model: 1\n",
      "Epoch: 5, train_loss: 0.59684392, valid_loss: 0.60728254, time: [4.95], best model: 1\n",
      "Epoch: 6, train_loss: 0.55295807, valid_loss: 0.56792149, time: [5.06], best model: 1\n",
      "Epoch: 7, train_loss: 0.52713923, valid_loss: 0.54593789, time: [5.09], best model: 1\n",
      "Epoch: 8, train_loss: 0.51157654, valid_loss: 0.52492553, time: [5.04], best model: 1\n",
      "Epoch: 9, train_loss: 0.4851974, valid_loss: 0.49291704, time: [4.96], best model: 1\n",
      "Epoch: 10, train_loss: 0.47548438, valid_loss: 0.4785599, time: [4.94], best model: 1\n",
      "Epoch: 11, train_loss: 0.45053281, valid_loss: 0.46039782, time: [4.99], best model: 1\n",
      "Epoch: 12, train_loss: 0.4382349, valid_loss: 0.45247573, time: [5.], best model: 1\n",
      "Epoch: 13, train_loss: 0.42791795, valid_loss: 0.44806674, time: [4.97], best model: 1\n",
      "Epoch: 14, train_loss: 0.4209411, valid_loss: 0.45155943, time: [4.95], best model: 0\n",
      "Epoch: 15, train_loss: 0.41514012, valid_loss: 0.43644256, time: [4.99], best model: 1\n",
      "Epoch: 16, train_loss: 0.39832035, valid_loss: 0.42801539, time: [5.05], best model: 1\n",
      "Epoch: 17, train_loss: 0.38899827, valid_loss: 0.41656046, time: [5.02], best model: 1\n",
      "Epoch: 18, train_loss: 0.38208737, valid_loss: 0.41165282, time: [5.07], best model: 1\n",
      "Epoch: 19, train_loss: 0.37738442, valid_loss: 0.41162581, time: [5.01], best model: 1\n",
      "Epoch: 20, train_loss: 0.37097847, valid_loss: 0.40140948, time: [5.01], best model: 1\n",
      "Epoch: 21, train_loss: 0.36286033, valid_loss: 0.39871668, time: [5.02], best model: 1\n",
      "Epoch: 22, train_loss: 0.36212473, valid_loss: 0.4032565, time: [4.95], best model: 0\n",
      "Epoch: 23, train_loss: 0.35824585, valid_loss: 0.40318482, time: [5.05], best model: 0\n",
      "Epoch: 24, train_loss: 0.35628095, valid_loss: 0.39824245, time: [4.98], best model: 1\n",
      "Epoch: 25, train_loss: 0.34693815, valid_loss: 0.3918618, time: [4.98], best model: 1\n",
      "Epoch: 26, train_loss: 0.34390446, valid_loss: 0.40341581, time: [5.05], best model: 0\n",
      "Epoch: 27, train_loss: 0.34031333, valid_loss: 0.4020983, time: [4.99], best model: 0\n",
      "Epoch: 28, train_loss: 0.33777635, valid_loss: 0.40726203, time: [4.94], best model: 0\n",
      "Epoch: 29, train_loss: 0.33173738, valid_loss: 0.40325724, time: [5.03], best model: 0\n",
      "Early Stopped at Epoch: 30\n",
      "On sample 6 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 72, 'learning_rate': 0.030786881587000436, 'num_epochs': 98, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.10174307637214693, 'seed': 7244})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (rl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (hl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=72, bias=True)\n",
      "  (fc): Linear(in_features=72, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.8774245, valid_loss: 0.88367995, time: [4.13], best model: 1\n",
      "Epoch: 1, train_loss: 0.82844912, valid_loss: 0.83242259, time: [4.21], best model: 1\n",
      "Epoch: 2, train_loss: 0.78812408, valid_loss: 0.78145172, time: [4.13], best model: 1\n",
      "Epoch: 3, train_loss: 0.74907882, valid_loss: 0.75580768, time: [4.04], best model: 1\n",
      "Epoch: 4, train_loss: 0.69522963, valid_loss: 0.72020287, time: [4.08], best model: 1\n",
      "Epoch: 5, train_loss: 0.66195179, valid_loss: 0.68290414, time: [4.08], best model: 1\n",
      "Epoch: 6, train_loss: 0.61568938, valid_loss: 0.64884304, time: [4.2], best model: 1\n",
      "Epoch: 7, train_loss: 0.60996088, valid_loss: 0.62171903, time: [4.07], best model: 1\n",
      "Epoch: 8, train_loss: 0.58208571, valid_loss: 0.60111921, time: [4.12], best model: 1\n",
      "Epoch: 9, train_loss: 0.56481421, valid_loss: 0.59703958, time: [4.2], best model: 1\n",
      "Epoch: 10, train_loss: 0.54041826, valid_loss: 0.57884104, time: [4.07], best model: 1\n",
      "Epoch: 11, train_loss: 0.53421626, valid_loss: 0.55399757, time: [4.12], best model: 1\n",
      "Epoch: 12, train_loss: 0.53064346, valid_loss: 0.54826904, time: [4.09], best model: 1\n",
      "Epoch: 13, train_loss: 0.51223505, valid_loss: 0.55295793, time: [4.08], best model: 0\n",
      "Epoch: 14, train_loss: 0.50952763, valid_loss: 0.53483759, time: [4.11], best model: 1\n",
      "Epoch: 15, train_loss: 0.48321573, valid_loss: 0.52085607, time: [4.18], best model: 1\n",
      "Epoch: 16, train_loss: 0.4935212, valid_loss: 0.5109642, time: [4.18], best model: 1\n",
      "Epoch: 17, train_loss: 0.4830125, valid_loss: 0.4942305, time: [4.1], best model: 1\n",
      "Epoch: 18, train_loss: 0.48123307, valid_loss: 0.50095344, time: [4.11], best model: 0\n",
      "Epoch: 19, train_loss: 0.46822725, valid_loss: 0.48090964, time: [4.14], best model: 1\n",
      "Epoch: 20, train_loss: 0.46368162, valid_loss: 0.49634907, time: [4.15], best model: 0\n",
      "Epoch: 21, train_loss: 0.45196694, valid_loss: 0.48954109, time: [4.15], best model: 0\n",
      "Epoch: 22, train_loss: 0.44977214, valid_loss: 0.48565463, time: [4.06], best model: 0\n",
      "Epoch: 23, train_loss: 0.445672, valid_loss: 0.47478978, time: [4.12], best model: 1\n",
      "Epoch: 24, train_loss: 0.44700073, valid_loss: 0.4651139, time: [4.06], best model: 1\n",
      "Epoch: 25, train_loss: 0.43675449, valid_loss: 0.47695058, time: [4.11], best model: 0\n",
      "Epoch: 26, train_loss: 0.43232477, valid_loss: 0.4663911, time: [4.1], best model: 0\n",
      "Epoch: 27, train_loss: 0.42988011, valid_loss: 0.47144347, time: [4.], best model: 0\n",
      "Epoch: 28, train_loss: 0.42771468, valid_loss: 0.46225304, time: [4.12], best model: 1\n",
      "Epoch: 29, train_loss: 0.41355722, valid_loss: 0.45968924, time: [4.23], best model: 1\n",
      "Epoch: 30, train_loss: 0.41212983, valid_loss: 0.45647569, time: [4.22], best model: 1\n",
      "Epoch: 31, train_loss: 0.40387387, valid_loss: 0.45281104, time: [4.16], best model: 1\n",
      "Epoch: 32, train_loss: 0.40439385, valid_loss: 0.4514168, time: [4.14], best model: 1\n",
      "Epoch: 33, train_loss: 0.39812742, valid_loss: 0.45057885, time: [4.2], best model: 1\n",
      "Epoch: 34, train_loss: 0.38276304, valid_loss: 0.44218803, time: [4.05], best model: 1\n",
      "Epoch: 35, train_loss: 0.37663644, valid_loss: 0.43752562, time: [4.08], best model: 1\n",
      "Epoch: 36, train_loss: 0.37677397, valid_loss: 0.43548114, time: [4.12], best model: 1\n",
      "Epoch: 37, train_loss: 0.3787167, valid_loss: 0.43217843, time: [4.12], best model: 1\n",
      "Epoch: 38, train_loss: 0.37131106, valid_loss: 0.4361053, time: [4.15], best model: 0\n",
      "Epoch: 39, train_loss: 0.366466, valid_loss: 0.43503939, time: [4.12], best model: 0\n",
      "Epoch: 40, train_loss: 0.35946178, valid_loss: 0.42634829, time: [4.09], best model: 1\n",
      "Epoch: 41, train_loss: 0.35518574, valid_loss: 0.42938976, time: [4.05], best model: 0\n",
      "Epoch: 42, train_loss: 0.34922692, valid_loss: 0.4280896, time: [4.13], best model: 0\n",
      "Epoch: 43, train_loss: 0.34294668, valid_loss: 0.42123824, time: [4.11], best model: 1\n",
      "Epoch: 44, train_loss: 0.33860203, valid_loss: 0.42519438, time: [4.06], best model: 0\n",
      "Epoch: 45, train_loss: 0.3393065, valid_loss: 0.41635816, time: [4.09], best model: 1\n",
      "Epoch: 46, train_loss: 0.33049847, valid_loss: 0.42264462, time: [4.12], best model: 0\n",
      "Epoch: 47, train_loss: 0.32930088, valid_loss: 0.41666064, time: [4.15], best model: 0\n",
      "Epoch: 48, train_loss: 0.32322558, valid_loss: 0.41795451, time: [4.11], best model: 0\n",
      "Early Stopped at Epoch: 49\n",
      "On sample 7 / 15 (hyperparams = {'cell_size': 68, 'hidden_size': 68, 'learning_rate': 0.013619331759609964, 'num_epochs': 111, 'patience': 3, 'batch_size': 64, 'early_stop_frac': 0.13844715976978939, 'seed': 4659})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (rl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (hl): Linear(in_features=276, out_features=68, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=68, bias=True)\n",
      "  (fc): Linear(in_features=68, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.78042175, valid_loss: 0.77620171, time: [13.53], best model: 1\n",
      "Epoch: 1, train_loss: 0.60070218, valid_loss: 0.59470323, time: [13.22], best model: 1\n",
      "Epoch: 2, train_loss: 0.49819645, valid_loss: 0.49841088, time: [13.39], best model: 1\n",
      "Epoch: 3, train_loss: 0.44891507, valid_loss: 0.44405182, time: [13.35], best model: 1\n",
      "Epoch: 4, train_loss: 0.41524126, valid_loss: 0.4123725, time: [13.18], best model: 1\n",
      "Epoch: 5, train_loss: 0.39185364, valid_loss: 0.38777812, time: [13.38], best model: 1\n",
      "Epoch: 6, train_loss: 0.37197164, valid_loss: 0.37732378, time: [13.36], best model: 1\n",
      "Epoch: 7, train_loss: 0.37463013, valid_loss: 0.37107351, time: [13.46], best model: 1\n",
      "Epoch: 8, train_loss: 0.36401086, valid_loss: 0.36748335, time: [13.13], best model: 1\n",
      "Epoch: 9, train_loss: 0.36743839, valid_loss: 0.3685257, time: [13.28], best model: 0\n",
      "Epoch: 10, train_loss: 0.36516656, valid_loss: 0.36701121, time: [13.23], best model: 1\n",
      "Epoch: 11, train_loss: 0.35355147, valid_loss: 0.36479062, time: [13.19], best model: 1\n",
      "Epoch: 12, train_loss: 0.35641774, valid_loss: 0.37062185, time: [13.26], best model: 0\n",
      "Epoch: 13, train_loss: 0.34969191, valid_loss: 0.36964837, time: [13.25], best model: 0\n",
      "Early Stopped at Epoch: 14\n",
      "New Best Score: 71.13 @ hyperparams = {'cell_size': 68, 'hidden_size': 68, 'learning_rate': 0.013619331759609964, 'num_epochs': 111, 'patience': 3, 'batch_size': 64, 'early_stop_frac': 0.13844715976978939, 'seed': 4659}\n",
      "On sample 8 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 71, 'learning_rate': 0.02017270377811787, 'num_epochs': 58, 'patience': 3, 'batch_size': 128, 'early_stop_frac': 0.10536338408272322, 'seed': 1226})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (rl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (hl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=71, bias=True)\n",
      "  (fc): Linear(in_features=71, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.82213443, valid_loss: 0.80900965, time: [8.68], best model: 1\n",
      "Epoch: 1, train_loss: 0.68982703, valid_loss: 0.68780537, time: [8.54], best model: 1\n",
      "Epoch: 2, train_loss: 0.60845315, valid_loss: 0.60264014, time: [8.71], best model: 1\n",
      "Epoch: 3, train_loss: 0.53791516, valid_loss: 0.54557598, time: [8.64], best model: 1\n",
      "Epoch: 4, train_loss: 0.50117871, valid_loss: 0.50415365, time: [8.6], best model: 1\n",
      "Epoch: 5, train_loss: 0.46508678, valid_loss: 0.48113792, time: [8.65], best model: 1\n",
      "Epoch: 6, train_loss: 0.44397126, valid_loss: 0.45472359, time: [8.52], best model: 1\n",
      "Epoch: 7, train_loss: 0.42021675, valid_loss: 0.43430733, time: [8.65], best model: 1\n",
      "Epoch: 8, train_loss: 0.40648159, valid_loss: 0.42146259, time: [8.58], best model: 1\n",
      "Epoch: 9, train_loss: 0.39516782, valid_loss: 0.40583889, time: [8.71], best model: 1\n",
      "Epoch: 10, train_loss: 0.376261, valid_loss: 0.39509638, time: [8.52], best model: 1\n",
      "Epoch: 11, train_loss: 0.36928301, valid_loss: 0.3938834, time: [8.65], best model: 1\n",
      "Epoch: 12, train_loss: 0.36222298, valid_loss: 0.38741886, time: [8.65], best model: 1\n",
      "Epoch: 13, train_loss: 0.36297927, valid_loss: 0.38491003, time: [8.52], best model: 1\n",
      "Epoch: 14, train_loss: 0.36035277, valid_loss: 0.3888913, time: [8.61], best model: 0\n",
      "Epoch: 15, train_loss: 0.35597059, valid_loss: 0.38510347, time: [8.53], best model: 0\n",
      "Early Stopped at Epoch: 16\n",
      "On sample 9 / 15 (hyperparams = {'cell_size': 58, 'hidden_size': 69, 'learning_rate': 0.05142897677179076, 'num_epochs': 47, 'patience': 4, 'batch_size': 256, 'early_stop_frac': 0.10736424951768658, 'seed': 6792})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (rl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (hl): Linear(in_features=277, out_features=69, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=69, bias=True)\n",
      "  (fc): Linear(in_features=69, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.85262384, valid_loss: 0.84757555, time: [5.15], best model: 1\n",
      "Epoch: 1, train_loss: 0.76559718, valid_loss: 0.76378829, time: [5.1], best model: 1\n",
      "Epoch: 2, train_loss: 0.69645033, valid_loss: 0.69546259, time: [5.11], best model: 1\n",
      "Epoch: 3, train_loss: 0.63830461, valid_loss: 0.64547447, time: [5.19], best model: 1\n",
      "Epoch: 4, train_loss: 0.5953502, valid_loss: 0.59961273, time: [5.06], best model: 1\n",
      "Epoch: 5, train_loss: 0.55736252, valid_loss: 0.55897312, time: [5.12], best model: 1\n",
      "Epoch: 6, train_loss: 0.53418317, valid_loss: 0.55495131, time: [5.14], best model: 1\n",
      "Epoch: 7, train_loss: 0.5217198, valid_loss: 0.52597546, time: [5.06], best model: 1\n",
      "Epoch: 8, train_loss: 0.50471569, valid_loss: 0.50966019, time: [5.12], best model: 1\n",
      "Epoch: 9, train_loss: 0.48207207, valid_loss: 0.48769698, time: [5.06], best model: 1\n",
      "Epoch: 10, train_loss: 0.4630247, valid_loss: 0.48092576, time: [5.08], best model: 1\n",
      "Epoch: 11, train_loss: 0.45944888, valid_loss: 0.47511193, time: [5.09], best model: 1\n",
      "Epoch: 12, train_loss: 0.4571257, valid_loss: 0.45430305, time: [5.15], best model: 1\n",
      "Epoch: 13, train_loss: 0.43401389, valid_loss: 0.44866131, time: [5.19], best model: 1\n",
      "Epoch: 14, train_loss: 0.43039763, valid_loss: 0.44623013, time: [5.16], best model: 1\n",
      "Epoch: 15, train_loss: 0.41754617, valid_loss: 0.43823801, time: [5.11], best model: 1\n",
      "Epoch: 16, train_loss: 0.40809931, valid_loss: 0.43404855, time: [5.12], best model: 1\n",
      "Epoch: 17, train_loss: 0.40336846, valid_loss: 0.42173372, time: [5.08], best model: 1\n",
      "Epoch: 18, train_loss: 0.39649306, valid_loss: 0.42137379, time: [5.14], best model: 1\n",
      "Epoch: 19, train_loss: 0.38479766, valid_loss: 0.41280076, time: [5.17], best model: 1\n",
      "Epoch: 20, train_loss: 0.37990468, valid_loss: 0.40723071, time: [5.14], best model: 1\n",
      "Epoch: 21, train_loss: 0.37015902, valid_loss: 0.4020074, time: [5.21], best model: 1\n",
      "Epoch: 22, train_loss: 0.36533362, valid_loss: 0.40190134, time: [5.1], best model: 1\n",
      "Epoch: 23, train_loss: 0.36025919, valid_loss: 0.40240252, time: [5.08], best model: 0\n",
      "Epoch: 24, train_loss: 0.35600439, valid_loss: 0.39711906, time: [5.13], best model: 1\n",
      "Epoch: 25, train_loss: 0.35447847, valid_loss: 0.40667445, time: [5.11], best model: 0\n",
      "Epoch: 26, train_loss: 0.34961894, valid_loss: 0.40248848, time: [5.17], best model: 0\n",
      "Epoch: 27, train_loss: 0.34184545, valid_loss: 0.40164875, time: [5.14], best model: 0\n",
      "Early Stopped at Epoch: 28\n",
      "On sample 10 / 15 (hyperparams = {'cell_size': 57, 'hidden_size': 75, 'learning_rate': 0.05857651292235553, 'num_epochs': 41, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08939332949520416, 'seed': 9043})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.75347291, valid_loss: 0.75841958, time: [14.32], best model: 1\n",
      "Epoch: 1, train_loss: 0.59445844, valid_loss: 0.58741696, time: [14.26], best model: 1\n",
      "Epoch: 2, train_loss: 0.49504721, valid_loss: 0.48335558, time: [14.22], best model: 1\n",
      "Epoch: 3, train_loss: 0.43277939, valid_loss: 0.42945772, time: [13.96], best model: 1\n",
      "Epoch: 4, train_loss: 0.39568114, valid_loss: 0.40123117, time: [14.08], best model: 1\n",
      "Epoch: 5, train_loss: 0.384019, valid_loss: 0.37947204, time: [14.13], best model: 1\n",
      "Epoch: 6, train_loss: 0.37662673, valid_loss: 0.37109641, time: [14.09], best model: 1\n",
      "Epoch: 7, train_loss: 0.37359811, valid_loss: 0.37218889, time: [14.15], best model: 0\n",
      "Epoch: 8, train_loss: 0.3651835, valid_loss: 0.36880089, time: [14.15], best model: 1\n",
      "Epoch: 9, train_loss: 0.36410785, valid_loss: 0.3661015, time: [14.08], best model: 1\n",
      "Epoch: 10, train_loss: 0.35295137, valid_loss: 0.36560325, time: [14.23], best model: 1\n",
      "Epoch: 11, train_loss: 0.3562767, valid_loss: 0.37045407, time: [14.22], best model: 0\n",
      "Epoch: 12, train_loss: 0.34956661, valid_loss: 0.3731375, time: [14.17], best model: 0\n",
      "Epoch: 13, train_loss: 0.34452089, valid_loss: 0.36815512, time: [14.06], best model: 0\n",
      "Early Stopped at Epoch: 14\n",
      "New Best Score: 72.26 @ hyperparams = {'cell_size': 57, 'hidden_size': 75, 'learning_rate': 0.05857651292235553, 'num_epochs': 41, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08939332949520416, 'seed': 9043}\n",
      "On sample 11 / 15 (hyperparams = {'cell_size': 52, 'hidden_size': 76, 'learning_rate': 0.024183516701270405, 'num_epochs': 91, 'patience': 5, 'batch_size': 256, 'early_stop_frac': 0.1426546750491463, 'seed': 1244})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (rl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (hl): Linear(in_features=284, out_features=76, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=76, bias=True)\n",
      "  (fc): Linear(in_features=76, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.86071239, valid_loss: 0.84238373, time: [5.23], best model: 1\n",
      "Epoch: 1, train_loss: 0.77021102, valid_loss: 0.77319683, time: [5.02], best model: 1\n",
      "Epoch: 2, train_loss: 0.70157037, valid_loss: 0.70382139, time: [5.02], best model: 1\n",
      "Epoch: 3, train_loss: 0.64272724, valid_loss: 0.65061147, time: [5.18], best model: 1\n",
      "Epoch: 4, train_loss: 0.60621623, valid_loss: 0.60387312, time: [4.78], best model: 1\n",
      "Epoch: 5, train_loss: 0.57283075, valid_loss: 0.58875057, time: [4.93], best model: 1\n",
      "Epoch: 6, train_loss: 0.55345644, valid_loss: 0.55894641, time: [4.98], best model: 1\n",
      "Epoch: 7, train_loss: 0.52823445, valid_loss: 0.53702044, time: [4.88], best model: 1\n",
      "Epoch: 8, train_loss: 0.51237917, valid_loss: 0.52413249, time: [4.88], best model: 1\n",
      "Epoch: 9, train_loss: 0.48488774, valid_loss: 0.496126, time: [4.77], best model: 1\n",
      "Epoch: 10, train_loss: 0.4765843, valid_loss: 0.47863664, time: [4.76], best model: 1\n",
      "Epoch: 11, train_loss: 0.46156406, valid_loss: 0.46707184, time: [4.76], best model: 1\n",
      "Epoch: 12, train_loss: 0.44431537, valid_loss: 0.4546349, time: [4.95], best model: 1\n",
      "Epoch: 13, train_loss: 0.43341698, valid_loss: 0.444234, time: [4.79], best model: 1\n",
      "Epoch: 14, train_loss: 0.42759871, valid_loss: 0.43494821, time: [4.78], best model: 1\n",
      "Epoch: 15, train_loss: 0.41433723, valid_loss: 0.42931168, time: [4.8], best model: 1\n",
      "Epoch: 16, train_loss: 0.39810893, valid_loss: 0.42125395, time: [4.72], best model: 1\n",
      "Epoch: 17, train_loss: 0.39642119, valid_loss: 0.41191261, time: [4.88], best model: 1\n",
      "Epoch: 18, train_loss: 0.38729497, valid_loss: 0.40734693, time: [4.89], best model: 1\n",
      "Epoch: 19, train_loss: 0.38251717, valid_loss: 0.39994277, time: [4.91], best model: 1\n",
      "Epoch: 20, train_loss: 0.37567861, valid_loss: 0.3903216, time: [4.82], best model: 1\n",
      "Epoch: 21, train_loss: 0.36770776, valid_loss: 0.3960088, time: [4.77], best model: 0\n",
      "Epoch: 22, train_loss: 0.35653315, valid_loss: 0.38627151, time: [4.79], best model: 1\n",
      "Epoch: 23, train_loss: 0.36172628, valid_loss: 0.38900195, time: [4.75], best model: 0\n",
      "Epoch: 24, train_loss: 0.35361017, valid_loss: 0.38899473, time: [4.75], best model: 0\n",
      "Epoch: 25, train_loss: 0.34699638, valid_loss: 0.38216257, time: [4.75], best model: 1\n",
      "Epoch: 26, train_loss: 0.34120846, valid_loss: 0.38290429, time: [4.76], best model: 0\n",
      "Epoch: 27, train_loss: 0.33482272, valid_loss: 0.38135801, time: [4.99], best model: 1\n",
      "Epoch: 28, train_loss: 0.33608978, valid_loss: 0.38531058, time: [4.85], best model: 0\n",
      "Epoch: 29, train_loss: 0.329372, valid_loss: 0.39365455, time: [4.92], best model: 0\n",
      "Epoch: 30, train_loss: 0.32154271, valid_loss: 0.39008321, time: [4.8], best model: 0\n",
      "Epoch: 31, train_loss: 0.32240701, valid_loss: 0.38851544, time: [4.82], best model: 0\n",
      "Early Stopped at Epoch: 32\n",
      "On sample 12 / 15 (hyperparams = {'cell_size': 67, 'hidden_size': 84, 'learning_rate': 0.07874911709152625, 'num_epochs': 55, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.050554441943176515, 'seed': 9967})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (rl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (hl): Linear(in_features=292, out_features=84, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=84, bias=True)\n",
      "  (fc): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.86229263, valid_loss: 0.85612039, time: [4.14], best model: 1\n",
      "Epoch: 1, train_loss: 0.81399419, valid_loss: 0.79153516, time: [4.04], best model: 1\n",
      "Epoch: 2, train_loss: 0.76957444, valid_loss: 0.76235599, time: [4.02], best model: 1\n",
      "Epoch: 3, train_loss: 0.71558577, valid_loss: 0.71529862, time: [4.33], best model: 1\n",
      "Epoch: 4, train_loss: 0.67106167, valid_loss: 0.6711806, time: [4.26], best model: 1\n",
      "Epoch: 5, train_loss: 0.63192706, valid_loss: 0.62660943, time: [4.15], best model: 1\n",
      "Epoch: 6, train_loss: 0.5998038, valid_loss: 0.61176122, time: [4.03], best model: 1\n",
      "Epoch: 7, train_loss: 0.5870664, valid_loss: 0.58674874, time: [4.13], best model: 1\n",
      "Epoch: 8, train_loss: 0.5701764, valid_loss: 0.57846949, time: [4.07], best model: 1\n",
      "Epoch: 9, train_loss: 0.55904511, valid_loss: 0.57509724, time: [4.02], best model: 1\n",
      "Epoch: 10, train_loss: 0.54983502, valid_loss: 0.55147122, time: [4.12], best model: 1\n",
      "Epoch: 11, train_loss: 0.52322511, valid_loss: 0.54619137, time: [4.57], best model: 1\n",
      "Epoch: 12, train_loss: 0.51405771, valid_loss: 0.54017147, time: [4.22], best model: 1\n",
      "Epoch: 13, train_loss: 0.50259089, valid_loss: 0.53080048, time: [4.09], best model: 1\n",
      "Epoch: 14, train_loss: 0.49106718, valid_loss: 0.52157814, time: [4.31], best model: 1\n",
      "Epoch: 15, train_loss: 0.49865529, valid_loss: 0.51485803, time: [4.56], best model: 1\n",
      "Epoch: 16, train_loss: 0.47806494, valid_loss: 0.5042297, time: [4.16], best model: 1\n",
      "Epoch: 17, train_loss: 0.48011715, valid_loss: 0.49322405, time: [4.29], best model: 1\n",
      "Epoch: 18, train_loss: 0.46069044, valid_loss: 0.49737487, time: [4.36], best model: 0\n",
      "Epoch: 19, train_loss: 0.46034238, valid_loss: 0.47797812, time: [4.24], best model: 1\n",
      "Epoch: 20, train_loss: 0.45819452, valid_loss: 0.47921688, time: [4.18], best model: 0\n",
      "Epoch: 21, train_loss: 0.44959019, valid_loss: 0.47432672, time: [4.51], best model: 1\n",
      "Epoch: 22, train_loss: 0.44896803, valid_loss: 0.47495094, time: [4.18], best model: 0\n",
      "Epoch: 23, train_loss: 0.43309086, valid_loss: 0.47021512, time: [4.31], best model: 1\n",
      "Epoch: 24, train_loss: 0.429197, valid_loss: 0.45591764, time: [4.12], best model: 1\n",
      "Epoch: 25, train_loss: 0.42487178, valid_loss: 0.45052199, time: [4.18], best model: 1\n",
      "Epoch: 26, train_loss: 0.41824944, valid_loss: 0.45725635, time: [4.13], best model: 0\n",
      "Epoch: 27, train_loss: 0.40984751, valid_loss: 0.44919011, time: [4.09], best model: 1\n",
      "Epoch: 28, train_loss: 0.40616543, valid_loss: 0.4414216, time: [4.1], best model: 1\n",
      "Epoch: 29, train_loss: 0.40075764, valid_loss: 0.43853286, time: [4.07], best model: 1\n",
      "Epoch: 30, train_loss: 0.40302366, valid_loss: 0.44237368, time: [4.1], best model: 0\n",
      "Epoch: 31, train_loss: 0.40007819, valid_loss: 0.43371939, time: [4.08], best model: 1\n",
      "Epoch: 32, train_loss: 0.39164036, valid_loss: 0.43597883, time: [4.24], best model: 0\n",
      "Epoch: 33, train_loss: 0.38363115, valid_loss: 0.4370515, time: [4.09], best model: 0\n",
      "Epoch: 34, train_loss: 0.37690061, valid_loss: 0.43696837, time: [4.08], best model: 0\n",
      "Early Stopped at Epoch: 35\n",
      "On sample 13 / 15 (hyperparams = {'cell_size': 61, 'hidden_size': 72, 'learning_rate': 0.0597308068795698, 'num_epochs': 49, 'patience': 4, 'batch_size': 512, 'early_stop_frac': 0.12284984962121943, 'seed': 8417})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (rl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (hl): Linear(in_features=280, out_features=72, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=72, bias=True)\n",
      "  (fc): Linear(in_features=72, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.87564223, valid_loss: 0.8824313, time: [3.75], best model: 1\n",
      "Epoch: 1, train_loss: 0.79838535, valid_loss: 0.823599, time: [3.69], best model: 1\n",
      "Epoch: 2, train_loss: 0.76198074, valid_loss: 0.78488418, time: [3.67], best model: 1\n",
      "Epoch: 3, train_loss: 0.73514026, valid_loss: 0.75054162, time: [3.66], best model: 1\n",
      "Epoch: 4, train_loss: 0.70723867, valid_loss: 0.72388349, time: [3.82], best model: 1\n",
      "Epoch: 5, train_loss: 0.66989361, valid_loss: 0.67786612, time: [3.93], best model: 1\n",
      "Epoch: 6, train_loss: 0.62899889, valid_loss: 0.65134764, time: [3.86], best model: 1\n",
      "Epoch: 7, train_loss: 0.62531185, valid_loss: 0.63588572, time: [3.76], best model: 1\n",
      "Epoch: 8, train_loss: 0.58170414, valid_loss: 0.60985443, time: [3.79], best model: 1\n",
      "Epoch: 9, train_loss: 0.57410383, valid_loss: 0.571655, time: [4.01], best model: 1\n",
      "Epoch: 10, train_loss: 0.5587022, valid_loss: 0.58167921, time: [3.84], best model: 0\n",
      "Epoch: 11, train_loss: 0.54186262, valid_loss: 0.56407421, time: [3.99], best model: 1\n",
      "Epoch: 12, train_loss: 0.52732461, valid_loss: 0.53993232, time: [3.76], best model: 1\n",
      "Epoch: 13, train_loss: 0.52617768, valid_loss: 0.54638859, time: [3.73], best model: 0\n",
      "Epoch: 14, train_loss: 0.49932367, valid_loss: 0.53054217, time: [3.69], best model: 1\n",
      "Epoch: 15, train_loss: 0.50959849, valid_loss: 0.5142812, time: [3.73], best model: 1\n",
      "Epoch: 16, train_loss: 0.49643224, valid_loss: 0.50770255, time: [3.72], best model: 1\n",
      "Epoch: 17, train_loss: 0.49054755, valid_loss: 0.50871236, time: [3.66], best model: 0\n",
      "Epoch: 18, train_loss: 0.46858658, valid_loss: 0.49316795, time: [3.68], best model: 1\n",
      "Epoch: 19, train_loss: 0.47599993, valid_loss: 0.50912414, time: [3.81], best model: 0\n",
      "Epoch: 20, train_loss: 0.45631266, valid_loss: 0.47480113, time: [3.74], best model: 1\n",
      "Epoch: 21, train_loss: 0.45112419, valid_loss: 0.48452715, time: [3.62], best model: 0\n",
      "Epoch: 22, train_loss: 0.45311557, valid_loss: 0.48842389, time: [3.81], best model: 0\n",
      "Epoch: 23, train_loss: 0.44953694, valid_loss: 0.49638782, time: [3.78], best model: 0\n",
      "Early Stopped at Epoch: 24\n",
      "On sample 14 / 15 (hyperparams = {'cell_size': 71, 'hidden_size': 71, 'learning_rate': 0.018782331070145576, 'num_epochs': 75, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.08557536288902531, 'seed': 50})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (rl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (hl): Linear(in_features=279, out_features=71, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=71, bias=True)\n",
      "  (fc): Linear(in_features=71, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.76631228, valid_loss: 0.75621805, time: [13.79], best model: 1\n",
      "Epoch: 1, train_loss: 0.57615617, valid_loss: 0.58323542, time: [13.76], best model: 1\n",
      "Epoch: 2, train_loss: 0.49573233, valid_loss: 0.49474137, time: [13.9], best model: 1\n",
      "Epoch: 3, train_loss: 0.43881653, valid_loss: 0.45060724, time: [13.78], best model: 1\n",
      "Epoch: 4, train_loss: 0.40547183, valid_loss: 0.41911233, time: [13.72], best model: 1\n",
      "Epoch: 5, train_loss: 0.38159084, valid_loss: 0.39460269, time: [13.7], best model: 1\n",
      "Epoch: 6, train_loss: 0.37242768, valid_loss: 0.3830914, time: [13.68], best model: 1\n",
      "Epoch: 7, train_loss: 0.37015004, valid_loss: 0.37968767, time: [14.12], best model: 1\n",
      "Epoch: 8, train_loss: 0.3617668, valid_loss: 0.37821437, time: [13.95], best model: 1\n",
      "Epoch: 9, train_loss: 0.36271086, valid_loss: 0.37796547, time: [13.7], best model: 1\n",
      "Epoch: 10, train_loss: 0.35977511, valid_loss: 0.36965432, time: [13.93], best model: 1\n",
      "Epoch: 11, train_loss: 0.36038039, valid_loss: 0.37041614, time: [13.91], best model: 0\n",
      "Epoch: 12, train_loss: 0.35636653, valid_loss: 0.37869183, time: [13.8], best model: 0\n",
      "Epoch: 13, train_loss: 0.34930544, valid_loss: 0.36961786, time: [13.77], best model: 1\n",
      "Epoch: 14, train_loss: 0.34044692, valid_loss: 0.3800188, time: [13.84], best model: 0\n",
      "Epoch: 15, train_loss: 0.34133767, valid_loss: 0.37682269, time: [14.07], best model: 0\n",
      "Epoch: 16, train_loss: 0.33424566, valid_loss: 0.36815177, time: [14.42], best model: 1\n",
      "Epoch: 17, train_loss: 0.33165511, valid_loss: 0.387863, time: [13.93], best model: 0\n",
      "Epoch: 18, train_loss: 0.32008633, valid_loss: 0.38876908, time: [13.74], best model: 0\n",
      "Epoch: 19, train_loss: 0.31280246, valid_loss: 0.39953907, time: [13.87], best model: 0\n",
      "Early Stopped at Epoch: 20\n",
      "On sample 15 / 15 (hyperparams = {'cell_size': 65, 'hidden_size': 75, 'learning_rate': 0.03874713686238469, 'num_epochs': 85, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.07129332368775368, 'seed': 297})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.76079519, valid_loss: 0.76457641, time: [14.21], best model: 1\n",
      "Epoch: 1, train_loss: 0.57948944, valid_loss: 0.58993913, time: [14.14], best model: 1\n",
      "Epoch: 2, train_loss: 0.48533213, valid_loss: 0.48471377, time: [14.62], best model: 1\n",
      "Epoch: 3, train_loss: 0.43180339, valid_loss: 0.4353622, time: [14.34], best model: 1\n",
      "Epoch: 4, train_loss: 0.39599487, valid_loss: 0.39680233, time: [14.22], best model: 1\n",
      "Epoch: 5, train_loss: 0.37789286, valid_loss: 0.38536653, time: [14.11], best model: 1\n",
      "Epoch: 6, train_loss: 0.37416532, valid_loss: 0.37210921, time: [14.27], best model: 1\n",
      "Epoch: 7, train_loss: 0.3689519, valid_loss: 0.37737365, time: [14.26], best model: 0\n",
      "Epoch: 8, train_loss: 0.35959902, valid_loss: 0.36964677, time: [14.08], best model: 1\n",
      "Epoch: 9, train_loss: 0.36236798, valid_loss: 0.36944316, time: [14.03], best model: 1\n",
      "Epoch: 10, train_loss: 0.35565813, valid_loss: 0.37213395, time: [13.95], best model: 0\n",
      "Epoch: 11, train_loss: 0.35561129, valid_loss: 0.36787967, time: [13.99], best model: 1\n",
      "Epoch: 12, train_loss: 0.34486691, valid_loss: 0.37844218, time: [13.92], best model: 0\n",
      "Epoch: 13, train_loss: 0.34481115, valid_loss: 0.37213571, time: [14.03], best model: 0\n",
      "Epoch: 14, train_loss: 0.33986353, valid_loss: 0.37689112, time: [14.05], best model: 0\n",
      "Early Stopped at Epoch: 15\n",
      "New Best Score: 72.40 @ hyperparams = {'cell_size': 65, 'hidden_size': 75, 'learning_rate': 0.03874713686238469, 'num_epochs': 85, 'patience': 4, 'batch_size': 64, 'early_stop_frac': 0.07129332368775368, 'seed': 297}\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (rl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (hl): Linear(in_features=283, out_features=75, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=75, bias=True)\n",
      "  (fc): Linear(in_features=75, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n",
      "Epoch: 0, train_loss: 0.74820844, valid_loss: 0.75304939, time: [16.05], best model: 1\n",
      "Epoch: 1, train_loss: 0.56003931, valid_loss: 0.56574654, time: [16.03], best model: 1\n",
      "Epoch: 2, train_loss: 0.46601592, valid_loss: 0.47017414, time: [15.91], best model: 1\n",
      "Epoch: 3, train_loss: 0.41555701, valid_loss: 0.41492746, time: [16.09], best model: 1\n",
      "Epoch: 4, train_loss: 0.39229135, valid_loss: 0.38686374, time: [16.43], best model: 1\n",
      "Epoch: 5, train_loss: 0.37774036, valid_loss: 0.37363015, time: [17.09], best model: 1\n",
      "Epoch: 6, train_loss: 0.37343102, valid_loss: 0.37153769, time: [16.64], best model: 1\n",
      "Epoch: 7, train_loss: 0.36705741, valid_loss: 0.36920868, time: [16.07], best model: 1\n",
      "Epoch: 8, train_loss: 0.36344092, valid_loss: 0.37255289, time: [16.44], best model: 0\n",
      "Epoch: 9, train_loss: 0.36695151, valid_loss: 0.36988572, time: [16.12], best model: 0\n",
      "Epoch: 10, train_loss: 0.36095966, valid_loss: 0.36566608, time: [16.36], best model: 1\n",
      "Epoch: 11, train_loss: 0.35402801, valid_loss: 0.36579901, time: [16.55], best model: 0\n",
      "Epoch: 12, train_loss: 0.35323202, valid_loss: 0.36939622, time: [16.19], best model: 0\n",
      "Epoch: 13, train_loss: 0.34747334, valid_loss: 0.37223871, time: [16.16], best model: 0\n",
      "Early Stopped at Epoch: 14\n",
      "Final results for model GRU-D on target los_7 with representation data\n",
      "0.7321556866892159 0.18170766147047457 0.9191300675675675 0.11954022988505747\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from mmd_grud_utils import *\n",
    "\n",
    "model_name       = 'GRU-D'\n",
    "hyperparams_list = GRU_D_hyperparams_list\n",
    "RERUN            = False\n",
    "results = {}\n",
    "\n",
    "if model_name not in results: \n",
    "    results[model_name] = {}\n",
    "\n",
    "for t in ['los_3', 'los_7']:\n",
    "    if t not in results[model_name]: \n",
    "        results[model_name][t] = {}\n",
    "    n, X_train, X_dev, X_test = ('data', data_train, data_dev, data_test)\n",
    "    \n",
    "    print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    tensor = to_3D_tensor(\n",
    "            X_train.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "            np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "        )\n",
    "    X_mean = np.nanmean(tensor, axis=0, keepdims=True).transpose([0, 2, 1])\n",
    "    X_mean[X_mean != X_mean] = 0  # THIS IS EXTREME BODGE. We should not set NaN values to 0, as that wrongly influences training\n",
    "    base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "    if n in results[model_name][t]:\n",
    "        if not RERUN: \n",
    "            print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "            print(results[model_name][t][n])\n",
    "            continue\n",
    "        best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "        print(\"Loading best hyperparams\", best_hyperparams)\n",
    "    \n",
    "    else:\n",
    "        best_s, best_hyperparams = -np.Inf, None\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "            early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "            batch_size = int(batch_size)\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[\n",
    "                Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "            ]\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(\n",
    "                X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "            )\n",
    "            dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "            test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model = GRUD(**model_hyperparams)\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "            probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "            probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "            labels_dev        = np.concatenate(labels_dev)\n",
    "            s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "            if s > best_s:\n",
    "                best_s, best_hyperparams = s, hyperparams\n",
    "                print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "                \n",
    "    ## Test set\n",
    "    np.random.seed(seed)\n",
    "    early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "\n",
    "    all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "    N_early_stop = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "    X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "    X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "\n",
    "    y_score = np.concatenate(probabilities_test)[:, 1]\n",
    "    y_pred  = np.concatenate(probabilities_test).argmax(axis=1)\n",
    "    y_true  = np.concatenate(labels_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    print(auc, auprc, acc, F1)\n",
    "\n",
    "    results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "    # with open('/scratch/mmd/extraction_baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
